---
title: "Spooky Author Identification"
author: |
  | Eugene Girtcius
  | Transport and Telecommunication Institute
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    css: styles.css
    df_print: kable
    fig_caption: yes
    fig_height: 6
    fig_width: 9
    highlight: tango
    mathjax: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
  # word_document:
  #   toc: yes
  #   toc_depth: '2'
  # pdf_document:
  #   dev: cairo_pdf
  #   df_print: kable
  #   fig_caption: yes
  #   fig_height: 6
  #   fig_width: 9
  #   highlight: tango
  #   includes:
  #     in_header: preamble-latex.tex
  #   keep_tex: yes
  #   latex_engine: xelatex
  #   number_sections: yes
  #   toc: yes
fontsize: 12pt
geometry: left=3cm, right=3cm, top=3cm, bottom=3cm
header-includes:
- \usepackage{longtable}
- \usepackage{natbib}
- \setcitestyle{square,numbers,super}
- \usepackage{amsmath}
# mainfont: TimesNewRomanPSMT
linkcolor: blue
documentclass: article
citecolor: blue
urlcolor: blue
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
      equationNumbers: {
            autoNumber: "all",
            <!-- formatNumber: function (n) {return '9.'+n} -->
      }
  }
});
</script>

```{r setup, include = F, echo = F}
knitr::opts_chunk$set(echo = T, error = F, cache.lazy = F)
# options(knitr.table.format = "latex", booktabs = T)
```

# Введение

[Spooky Author Identification](https://www.kaggle.com/c/spooky-author-identification/)

**ЦЕЛЬЮ** исследования является попытка предсказать автора по короткому отрывку из написанного им текста методами ML. В данных присутствуют 3 автора: Г.Ф. Лавкрафт (HPL), Э.А. По (EAP) и М.В. Шелли (MWS)

**ЗАДАЧИ**:

 * Проведение EDA
 * Выявление признаков для классификации
 * Построение нескольких моделей для получения предсказаний.
 * Выбор Квазиоптимальной модели (ансамбля)
 * Попадание в ТОП 20% в рейтинге Kaggle
 * Освоение инструментов для дипломной работы

В качестве основных подходов в работе будет рассмотрен *XGBoost*, нейронные сети(*LSTM*, *FastText*, *BERT*) и *наивный байесовский классификатор*. Для написания кода будет использоваться язык **R**.

Несмотря на то, что работа построена вокруг анализа текста, задача классификации авторов не требует специализированной модели, а базируется на качественной подготовке входных признаков. Этапу извлечения признаков и анализу данных в целом будет уделено основное внимание. Также на это решение повлияло сходство этих этапов для широкого круга задач, в том числе и для будущей магистерской работы предобработка и извлечение признаков будут строиться по похожей схеме. Часть из найденных признаков, возможно, далее использоваться не будет и приведена для пополнения инструментария.

Следует также отметить, что признаки для работы с текстом, исходя из вышесказанного, в большинстве задач уже встречались и, поскольку, это первый опыт работы с текстом, они будут частично позамимствованы в интересных решениях, встреченных на просторах Kaggle. Некоторым оправданием такого рода плагиату могут служить, во-первых, возраст (часть написанного кода или не работает, или не особенно оптимально написана), а, во-вторых, язык (подавляющее большинство написано на python). 

Работа с моделями будет заключаться в получении предсказаний от достаточно большого числа не самых качественных моделей (порядка 10-ти) и построение поверх них классификатора, принимающего на вход эти предсказания в качестве дополнительных признаков. Предполагается, что это позволит достичь достаточно высоких позиций в соревновании (попадание в топ 20%). В случае, если это предположение не оправдается, будет выбрана одна из моделей (вероятнее всего, *XGBoost*) для дальнейшей оптимизации. Теи не менее, во многих случаях (на Kaggle), ансамбль средних моделей работает лучше одной оптимизированной.

С исходными данными можно ознакомиться [здесь](https://www.kaggle.com/c/spooky-author-identification/data). Данные представлены в виде тренировочного (`../input/train.csv`) и тестового набора  (`../input/test.csv`). Каждое наблюдение содержит короткий фрагмент текста (Как правило, предложение). Также представлен образец данных в формате, необходимой для загрузки в Kaggle.

# Подготовка {.tabset .tabset-fade .tabset-pills}

## Загрузка библиотек

В работе использовались классические библиотеки машинного обучения: *keras*, *caret*, *xgboost*. Для разметки и манипуляций с текстом выбраны *tidytext*, *udpipe*, *tm*, *wordcloud*. Визуализация проводилась через *ggplot*, обработка данных средствами *tidyverse*. Полный список библиотек приведен ниже.

```{r, message = F}
# general visualisation
library('ggplot2') # visualisation
library('scales') # visualisation
library('grid') # visualisation
library('gridExtra') # visualisation
library('RColorBrewer') # visualisation
library('corrplot') # visualisation

# general data manipulation
library('dplyr') # data manipulation
library('readr') # input/output
library('data.table') # data manipulation
library('tibble') # data wrangling
library('tidyr') # data wrangling
library('stringr') # string manipulation
library('forcats') # factor manipulation

# specific visualisation
library('alluvial') # visualisation
library('ggrepel') # visualisation
library('ggridges') # visualisation
library('gganimate') # visualisation
library('ggExtra') # visualisation

# specific data manipulation
library('lazyeval') # data wrangling
library('broom') # data wrangling
library('purrr') # string manipulation
library('reshape2') # data wrangling

# Text / NLP
library('tidytext') # text analysis
library('tm') # text analysis
library('SnowballC') # text analysis
library('topicmodels') # text analysis
library('wordcloud') # test visualisation
library('igraph') # visualisation
library('ggraph') # visualisation
library('babynames') # names

# Models
library('Matrix')
library('xgboost')
library('caret')

library('treemapify') #visualisation

require(tidyverse)
require(tidytext)
require(textstem)
require(qdap)
require(caret)
require(widyr)
require(broom)
require(keras)
require(gridExtra)
require(plotly)
require(scales)
require(ggcorrplot)
# require(RDRPOSTagger)
require(parallel)
require(gmodels)
require(knitr)
require(udpipe)
require(zoo)
require(fastNaiveBayes)
require(e1071)
require(tibbletime)
# require(plotly)
# require(kableExtra)
```

## загрузка данных

```{r warning = F, results = F, message = F}
train <- read_csv('data/train.csv')
test <- read_csv('data/test.csv')
sample <- read.csv('data/sample_submission.csv')
```

# Анализ и визуализация данных

## Структура данных {.tabset .tabset-fade .tabset-pills}

Набор данных содержит 3 колонки, id, непосредственно текст и метку автора. В тренировочных данных 19579 наблюдений. Встречаются, как короткие, так и длинные отрывки. Приведенный отрывок максимальной длины похож на ошибку в разметке. Точки между предложенриями отсутствуют и данные попали в один отрывок. Возможно, это следует считать выбросом. В тестовых данных 8392 наблюдений и отсутствуют метки классов. Также присутствуют длинные отрывки с отсутствующими точками. Пропушенных данных в наблюдениях не обнаружено.

### Тренировочные данные

Первые 5 наблюдений

```{r, cache = T}
train %>%
    slice(1:5)
```

```{r, cache = T}
glimpse(train)
```

```{r, cache = T}
summary(train)
```

Отрывок максимальной длины

```{r, cache = T}
train %>%
    slice(which.max(nchar(text)))
```

Отрывок минимальной длины

```{r, cache = T}
train %>%
    slice(which.min(nchar(text)))
```

Пропущенные значения

```{r, cache = T}
colSums(is.na(train))
```

### Тестовые данные

Первые 5 наблюдений
 
```{r, cache = T}
test %>%
    slice(1:5)
```

```{r, cache = T}
glimpse(test)
```

```{r, cache = T}
summary(test)
```

Отрывок максимальной длины

```{r, cache = T}
test %>%
    slice(which.max(nchar(text)))
```

Отрывок минимальной длины

```{r, cache = T}
test %>%
    slice(which.min(nchar(text)))
```

Пропущенные значения

```{r, cache = T}
colSums(is.na(test))
```

## Пропорции

Тексты По занимают 40%, Шелли 31%, Лавкрафта 29%. Длина предложений у По немного короче, чем у остальных.

```{r, cache = T}
table(train$author) %>%
    prop.table()
```

```{r, cache = T}
train %>% 
    group_by(author) %>%
    summarise(n = n()) %>% 
    ggplot(data = ., aes(x = author, y = n, fill = author)) +
    geom_col(show.legend = F) +
    scale_fill_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
    xlab(label = 'author') +
    ylab(label = 'number of texts') +
    theme_bw()
```

```{r, cache = T}

train %>% 
    mutate(len = nchar(text)) %>% 
    ggplot(data = ., aes(x = len, fill = author)) +
    geom_histogram(binwidth = 50) +
    scale_fill_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
    facet_grid(. ~ author) +
    xlab(label = 'number of characters') +
    ylab(label = 'number of texts') +
    theme_bw()

```

Медиана выглядит предпочтительнее среднего, вследствие ряда длинных отрывков с пропущеннными точками.

```{r, cache = T}
train %>% 
    mutate(len = nchar(text)) %>% 
    group_by(author) %>% 
    summarise(`median text length` = median(len), `mean text length` = mean(len))
```

## Облака слов {.tabset .tabset-fade .tabset-pills}

Облако слов, - удобное представление наиболее часто встречаюшихся слов в тексте. Размер отражает частоту встречаемости. Для начала применим облако к тексту в целом.

```{r, cache = T}
train %>%
    unnest_tokens(word, text) %>% 
    count(word) %>% 
    with(wordcloud(word, n, max.words = 50, scale = c(3, 1.5), random.order = F, rot.per = 0, color = 'black'))
```

Ожидаемо, вспомогательные части речи, - союзы, артикли, местоимения etc в тексте встречаются чаще. Стоит ли от них избавляться при построении модели? Важность признаков будет рассмотрена позже, на этапе feature engineering. Ниже представлен вариант облака без учета вспомогательных слов.

```{r, cache = T}
train_clean <- train %>% 
    unnest_tokens(word, text) %>% 
    anti_join(stop_words, by = 'word')

train_clean %>%
    count(word) %>% 
    with(wordcloud(word, n, max.words = 50, scale = c(3, 1), random.order = F, rot.per = 0, color = 'black'))
```

Ключевые слова подтвеждают заявленную тематику текстов, - жизнь и смерть, время и страх. Прослеживаются детективные и даже романтические нотки. Попробуем разделить облака по авторам.

Есть как общие мотивы, например, тематика времени близка всем троим, так и индивидуальные особенности. Тексты Лавкрафта ближе всего к классическому пониманию ужасов. У По можно заметить склонность к детективному стилю. Шелли не чужда романтичность.

### Лавкрафт

```{r, cache = T}
train_clean %>%
    filter(author == 'HPL') %>% 
    count(word) %>% 
    with(wordcloud(word, n, max.words = 50, scale = c(3, 1), random.order = F, rot.per = 0, color = 'blue4'))
```

### По

```{r, cache = T}
train_clean %>%
    filter(author == 'EAP') %>% 
    count(word) %>% 
    with(wordcloud(word, n, max.words = 50, scale = c(3, 1), random.order = F, rot.per = 0, color = 'red4'))
```

### Шелли

```{r, cache = T}
train_clean %>%
    filter(author == 'MWS') %>% 
    count(word) %>% 
    with(wordcloud(word, n, max.words = 50, scale = c(3, 1), random.order = F, rot.per = 0, color = 'purple4'))
```

## Ключевые слова {.tabset .tabset-fade .tabset-pills}

Более классическое представление о частоте встречаемости можно построить, используя столбчатые диаграммы. Ниже представлены две диаграммы, первая построена по словам, для второй предварительно использовался стемминг.

Суда по частоте встреч имен Raymond и Perdita у Шелли, можно предположить, что значительная часть ее отрывков взята из одного произведения. Поможет ли это в дальнейшей классификации?

```{r, cache = T}
get_bar_plot <- function(author_id, clr, stem = F){
    
    if(stem){
        t <- train_clean %>%
            filter(author == author_id) %>% 
            mutate(word = wordStem(word))
    }else{
        t <- train_clean %>%
            filter(author == author_id)
    }
    
    t %>% 
        count(word) %>% 
        top_n(30, n) %>%
        arrange(n) %>%
        mutate(word = factor(word, levels = word)) %>%
        ggplot() +
        geom_col(aes(word, n), fill = clr) +
        ggtitle(author_id) +
        coord_flip()
}
```

### ключевые слова

```{r, cache = T}
pl1 <- get_bar_plot(author_id = 'HPL', clr = 'blue4')
pl2 <- get_bar_plot(author_id = 'EAP', clr = 'red4')
pl3 <- get_bar_plot(author_id = 'MWS', clr = 'purple4')

grid.arrange(pl1, pl2, pl3, nrow = 1)
```

### ключевые слова + стемминг

```{r, cache = T}
pl1 <- get_bar_plot(author_id = 'HPL', clr = 'blue4', stem = T)
pl2 <- get_bar_plot(author_id = 'EAP', clr = 'red4', stem = T)
pl3 <- get_bar_plot(author_id = 'MWS', clr = 'purple4', stem = T)

grid.arrange(pl1, pl2, pl3, nrow = 1)
```

## Уникальные слова

Лексикон Шелли, предоставленный в наборе данных заметно меньше, чем у остальных. Возможно это связано с установленным ранее фактом о больших заимствованиях из одного произведения.
```{r, cache = T}
train_clean %>%
    distinct(author, word) %>% 
    count(author) %>% 
    rename(`unique word count` = n)
```

```{r, cache = T}
get_uniq_words_plot <- function(author_id){
    
    t <- train_clean %>% 
        filter(author == author_id) %>% 
        count(word) %>% 
        arrange(desc(n))
    
    t %>% 
        mutate(cumsum = cumsum(n),
               cumsum_perc = round(100 * cumsum/sum(n), digits = 2)) %>% 
        ggplot(aes(x = 1:nrow(t), y = cumsum_perc)) +
        geom_line() +
        geom_hline(yintercept = 50, color = 'black', alpha = 0.5) +
        geom_hline(yintercept = 75, color = 'yellow', alpha = 0.5) +
        geom_hline(yintercept = 90, color = 'orange', alpha = 0.5) +
        geom_hline(yintercept = 95, color = 'red', alpha = 0.5) +
        xlab('number of unique words') +
        ylab('% coverage') +
        ggtitle(author_id) +
        theme_bw()
        
}

pl1 <- get_uniq_words_plot(author_id = 'HPL')
pl2 <- get_uniq_words_plot(author_id = 'EAP')
pl3 <- get_uniq_words_plot(author_id = 'MWS')

grid.arrange(pl1, pl2, pl3, nrow = 1)
```

Как видно из графиков, для По и Лавкрафта достаточно примерно 7500 слов, чтобы перекрыть 90% их отрывков. Для Шелли достаточно примерно 5500 слов.

## Частота совстречаемости слов {.tabset .tabset-fade .tabset-pills}

Слова, расположенные близко к пунктирной линии, встречаются с одинаковой частотой у авторов. Для данной задачи они не особенно интересны. Чем слово дальше отстоит от линии, тем больший перекос в его использовании у одного автора.

```{r, cache = T}
word_freq <- train_clean %>%
    count(author, word) %>%
    group_by(author)  %>%
    mutate(prop = n / sum(n)) %>% 
    select(-n) %>% 
    spread(author, prop)

get_word_freq_plot <- function(author_id1, author_id2){
    word_freq %>% 
        filter(!is.na(!!author_id1) & !is.na(!!author_id2)) %>% 
        mutate(clr = abs(!!author_id1 - !!author_id2)) %>%
        ggplot(aes(x = !!author_id1, y = !!author_id2, color = clr)) +
        geom_abline(color = 'gray40', lty = 2) +
        geom_jitter(alpha = 0.1, size = 2, width = 0.3, height = 0.3) +
        geom_text(aes(label = word), check_overlap = T, vjust = 1.5) +
        scale_x_log10(labels = percent_format()) +
        scale_y_log10(labels = percent_format()) +
        theme_bw() +
        theme(legend.position = 'none') +
        labs(x = author_id1, y = author_id2)
}
```

### Лавкрафт и По

```{r, cache = T}
get_word_freq_plot(author_id1 = quo(HPL), author_id2 = quo(EAP))
```

### Лавкрафт и Шелли

```{r, cache = T}
get_word_freq_plot(author_id1 = quo(HPL), author_id2 = quo(MWS))
```

### По и Шелли

```{r, cache = T}
get_word_freq_plot(author_id1 = quo(EAP), author_id2 = quo(MWS))
```

## Корреляция частот совстречаемости

Также стоит посмотреть на результаты корреляционного анализ частот совстречаемости. Еще раз можно убедиться, что у авторов много общего.

```{r, cache = T}
word_freq %>%
    select(-word) %>%
    cor(use = 'complete.obs', method = 'pearson') %>%
    corrplot(type = 'lower',
             method = 'number',
             diag = F)
```

## TF-IDF {.tabset .tabset-fade .tabset-pills}

[TF](https://ru.wikipedia.org/wiki/TF-IDF) (term frequency — частота слова) — отношение числа вхождений некоторого слова к общему числу слов документа. Таким образом, оценивается важность слова $t_i$ в пределах отдельного документа.

\begin{align}
    tf(i,d) = \frac{n_{t}}{\sum_k n_{k}} 
\end{align}

где $n_t$ есть число вхождений слова $t$ в документ, а в знаменателе — общее число слов в данном документе.

IDF (inverse document frequency — обратная частота документа) — инверсия частоты, с которой некоторое слово встречается в документах коллекции. Учёт IDF уменьшает вес широкоупотребительных слов. Для каждого уникального слова в пределах конкретной коллекции документов существует только одно значение IDF.

\begin{align}
    idf(t,D) = \mbox{log} \frac{|D|}{|\{d_i \in D \ | \ t \in d_i\}|}
\end{align}

где

* $D$ — число документов в коллекции;
* ${|\{d_i \in D \ | \ t \in d_i\}|}$ — число документов из коллекции $D$, в которых встречается $t$ (когда $n_t \neq 0$).

Таким образом, мера TF-IDF является произведением двух сомножителей:

\begin{align}
    tf\mbox- idf(t,d,D) = tf(t,d) \times idf(t,D)
\end{align}

Большой вес в TF-IDF получат слова с высокой частотой в пределах конкретного документа и с низкой частотой употреблений в других документах. Наиболее высокие значения имеют имена собственные. Можно сделать вывод, что Шелли более склонна к их использованию в тексте. Также можно предположить, что эта мера окажется значимой при классификации.

```{r, cache = T}
tf_idf <- train_clean %>%
    count(author, word) %>% 
    bind_tf_idf(word, author, n)

get_tf_idf_plot <- function(df, cnt){
    df %>%
        arrange(desc(tf_idf)) %>%
        mutate(word = factor(word, levels = rev(unique(word)))) %>%
        top_n(cnt, tf_idf) %>%
        ggplot(aes(word, tf_idf, fill = author)) +
        scale_fill_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
        geom_col() +
        labs(x = NULL, y = 'TF-IDF values') +
        theme_bw() +
        theme(legend.position = 'top') +
        coord_flip()
}

get_tf_idf_facet_plot <- function(df, cnt){
    df %>%
        arrange(desc(tf_idf)) %>%
        mutate(word = factor(word, levels = rev(unique(word)))) %>%
        group_by(author) %>%
        top_n(cnt, tf_idf) %>%
        ungroup() %>% 
        ggplot(aes(word, tf_idf, fill = author)) +
        scale_fill_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
        geom_col() +
        labs(x = NULL, y = 'TF-IDF values') +
        theme_bw() +
        theme(legend.position = 'none') +
        facet_wrap(~author, ncol = 3, scales = 'free_y') +
        coord_flip()
}
```

### общие униграммы

```{r, cache = T}
get_tf_idf_plot(tf_idf, 20)
```

### униграммы по авторам

```{r, cache = T}
get_tf_idf_facet_plot(tf_idf, 20)
```

## Биграммы {.tabset .tabset-fade .tabset-pills}

Помимо одиночных слов важное значение могут иметь словосочетания, в частности би- и триграммы. Помимо имен собственных здесь начинают появляться характерные сочетания. Как ни странно, и По, и Лавкрафт, оказывается, любили посмеяться. Шелли неравнодушна к сочетаниям, начинающимся на *dear*. По вспоминает каких-то многочисленных *madame*, а Лавкрафт нагоняет жути своими *lurking fear* и *ancient house*

```{r, cache = T}
train_clean_bigram <- train %>% 
    unnest_tokens(word, text, token = 'ngrams', n = 2) %>% 
    separate(word, c('word1', 'word2'), sep = ' ') %>% 
    anti_join(stop_words, by = c('word1' = 'word')) %>% 
    anti_join(stop_words, by = c('word2' = 'word')) %>% 
    unite(word, c('word1', 'word2'), sep = ' ')

tf_idf_bigram <- train_clean_bigram %>%
    count(author, word) %>% 
    bind_tf_idf(word, author, n)
```

### общие биграммы

```{r, cache = T}
get_tf_idf_plot(tf_idf_bigram, 20)
```

### биграммы по авторам

```{r, cache = T}
get_tf_idf_facet_plot(tf_idf_bigram, 15)
```

## Триграммы {.tabset .tabset-fade .tabset-pills}

По и Лавкрафт продолжают смеяться, причем По умудряется это делать по-разному, в свободное время подкидывая иностранных словечек. Лавкрафт на все лады вспоминает безумного араба аль Хазреда, а любимый Шелли Реймонд, оказывается, связан со вселенной [*доктора Кто*](https://en.wikipedia.org/wiki/Time_Lord). Интересные открытия.

```{r, cache = T}
train_clean_trigram <- train %>% 
    unnest_tokens(word, text, token = 'ngrams', n = 3) %>% 
    separate(word, c('word1', 'word2', 'word3'), sep = ' ') %>% 
    anti_join(stop_words, by = c('word1' = 'word')) %>% 
    anti_join(stop_words, by = c('word2' = 'word')) %>% 
    anti_join(stop_words, by = c('word3' = 'word')) %>% 
    unite(word, c('word1', 'word2', 'word3'), sep = ' ')

tf_idf_trigram <- train_clean_trigram %>%
    count(author, word) %>% 
    bind_tf_idf(word, author, n)
```

### общие триграммы

```{r, cache = T}
get_tf_idf_plot(tf_idf_trigram, 10)
```

### триграммы по авторам

```{r, cache = T}
get_tf_idf_facet_plot(tf_idf_trigram, 6)
```

## LDA

Рассматривая задачу сегрегации текста нельзя не остановиться на еще одном популярном подходе, [**LDA**](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation). Латентное размещение Дирихле — применяемая в машинном обучении и информационном поиске порождающая модель, позволяющая объяснять результаты наблюдений с помощью неявных групп, благодаря чему возможно выявление причин сходства некоторых частей данных. Например, если наблюдениями являются слова, собранные в документы, утверждается, что каждый документ представляет собой смесь небольшого количества тем и что появление каждого слова связано с одной из тем документа. LDA является одним из методов тематического моделирования.
Тематическое моделирование предназначено для нахождения похожих темы в разных документах и группировки разных слов так, чтобы каждая тема состояла из слов с одинаковым значением.

Число топиков выбирается произвольно. Для начала разумно проверить гипотезу о том, что каждого автора можно описать одним топиком.

```{r, cache = T}

train_dtm <-  train %>% 
    mutate(text = as.character(text), id = as.character(id), id = paste(author, id, sep="_")) %>% 
    unnest_tokens(word, text) %>% 
    anti_join(stop_words, by = 'word') %>% 
    count(id, word) %>% 
    cast_dtm(id, word, n)

train_lda <- LDA(train_dtm, k = 3, control = list(seed = 42))

train_topics <- tidy(train_lda, matrix = 'beta')

train_topics %>%
    group_by(topic) %>%
    top_n(20, beta) %>%
    ungroup() %>%
    arrange(topic, -beta) %>%
    mutate(term = reorder(term, beta)) %>%
    ggplot(aes(term, beta, fill = factor(topic))) +
    geom_col(show.legend = F) +
    facet_wrap(~ topic, scales = 'free', ncol = 5) +
    coord_flip() +
    theme_bw()
```


```{r, cache = T}
gamma_extractor <- function(lda_model) {
  tidy(lda_model, matrix="gamma") %>%
    spread(topic, gamma) %>%
    separate(document, c("author", "id"),
             sep="_",
             convert=TRUE)         %>%
    mutate(top = NA)               %>%
    mutate(prediction = NA)
}

gamma_lda <- gamma_extractor(train_lda)
length_lda <- nrow(gamma_lda)

for (i in 1:length_lda) {
  gamma_lda$top[i]        <- max(gamma_lda[i, 3:5])
  gamma_lda$prediction[i] <- match(gamma_lda[i,6], gamma_lda[i,3:5])
}

table(gamma_lda$author, gamma_lda$prediction)
```

Авторы практически равномерно распределились по топикам. Очевидно, по трем топикам идентификация автора не имеет смысла. Попробуем увеличить их число до 9.

```{r, cache = T}
train_lda <- LDA(train_dtm, k = 9, control = list(seed = 42))

train_topics <- tidy(train_lda, matrix = 'beta')

train_topics %>%
    group_by(topic) %>%
    top_n(7, beta) %>%
    ungroup() %>%
    arrange(topic, -beta) %>%
    mutate(term = reorder(term, beta)) %>%
    ggplot(aes(term, beta, fill = factor(topic))) +
    geom_col(show.legend = F) +
    facet_wrap(~ topic, scales = 'free', ncol = 3) +
    coord_flip() +
    theme_bw()
```

```{r, cache = T}
gamma_lda <- gamma_extractor(train_lda)
length_lda <- nrow(gamma_lda)

for (i in 1:length_lda) {
  gamma_lda$top[i]        <- max(gamma_lda[i, 3:11])
  gamma_lda$prediction[i] <- match(gamma_lda[i,12], gamma_lda[i,3:11])
}

table(gamma_lda$author, gamma_lda$prediction)
```

По сравнению с тремя топиками, особых изменений не обнаруживается. Да, в первом и четвертом топике присутствует *Raymond*, что должно указывать на Шелли, но остальные авторы в нем тоже присутствуют. К тому же, ключевые слова слишком сильно пересекаются между топиками, например, *time* присутствует в четырех, а *day* в пяти. Скорее всего, здесь сыграл роль общий жанр, в котором работали авторы.

# Feature engineering

## Стилометрические признаки

В качестве основных признаков можно рассматривать следующее:

- количество слов
- количество символов
- среднее количество символов в слове
- количество слогов
- среднее количество слогов в слове
- количество односложных слов
- количество многосложных слов
- количество слов, начинающихся с прописной буквы
- количество слов, в которых все буквы прописные
- количество уникальных слов
- количество стоп-слов
- количество длинных слов (больше n символов)
- количество пунктуационных символов

Для извлечения слогов используется функция *syllable_sum*, которая довольно капризно относчится к входным данным. В частности, она выдает ошибки при пропущенных значениях. При обычной конвертации пара строк как раз оказыватся в таком виде (где в исходном тексте были редкие символы, например, *'ΥΠΝΟΣ*). Поэтому выполнять придется с ручной очисткой.

```{r, cache = T}

train_stylometry <- train %>% 
    mutate(ascii_text = iconv(text, to='ASCII//TRANSLIT'),
           ascii_text = ifelse(is.na(ascii_text), text, ascii_text),
           ascii_text = str_replace_all(ascii_text, c("ΥΠΝΟΣ" = "Hypnos", "Οἶδα Οἶδα" = "Oida Oida")),
           word_count = str_count(text, '\\w+'),
           nchar = nchar(text),
           nchar_per_word = nchar/word_count,
           syll_count = syllable_sum(ascii_text),
           nsyll_per_word = syll_count/word_count,
           tc_count = str_count(text, '[A-Z][a-z]+'),
           uc_count = str_count(text, '[A-Z][A-Z]+'),
           punctuation_count = str_count(text, '[:punct:]')) %>% 
    select(-c(text, ascii_text, author))

test_stylometry <- test %>% 
    mutate(ascii_text = iconv(text, to='ASCII//TRANSLIT'),
           word_count = str_count(text, '\\w+'),
           nchar = nchar(text),
           nchar_per_word = nchar/word_count,
           syll_count = syllable_sum(ascii_text),
           nsyll_per_word = syll_count/word_count,
           tc_count = str_count(text, '[A-Z][a-z]+'),
           uc_count = str_count(text, '[A-Z][A-Z]+'),
           punctuation_count = str_count(text, '[:punct:]')) %>% 
    select(-c(text, ascii_text))
 
```

```{r, cache = T}
 
train_tmp <- train %>%
    unnest_tokens(term, text, token = 'ngrams', n=1) %>%
    mutate(stop_word_ind = as.integer(term %in% stop_words$word),
           ascii_term = iconv(term, to='ASCII//TRANSLIT'),
           ascii_term = ifelse(is.na(ascii_term), term, ascii_term),
           ascii_term = str_replace_all(ascii_term, c("υπνος" = "hypnos", "οἶδα" = "oida")),
           syll_count = syllable_sum(ascii_term, parallel = T),
           word_length = nchar(term)) %>%
    group_by(id) %>%
    summarise(nStopWord = sum(stop_word_ind),
              nUniqueWord = n_distinct(term),
              avg_word_length = mean(word_length),
              ngt6lWord = sum(word_length > 6),
              n1SylWord = sum(syll_count == 1),
              nPolySylWord = sum(syll_count > 2)
              )

test_tmp <- test %>%
    unnest_tokens(term, text, token = 'ngrams', n=1) %>%
    mutate(stop_word_ind = as.integer(term %in% stop_words$word),
           ascii_term = iconv(term, to='ASCII//TRANSLIT'),
           ascii_term = ifelse(is.na(ascii_term), term, ascii_term),
           ascii_term = str_replace_all(ascii_term, c("υπνος" = "hypnos", "οἶδα" = "oida")),
           syll_count = syllable_sum(ascii_term, parallel = T),
           word_length = nchar(term)) %>%
    group_by(id) %>%
    summarise(nStopWord = sum(stop_word_ind),
              nUniqueWord = n_distinct(term),
              avg_word_length = mean(word_length),
              ngt6lWord = sum(word_length > 6),
              n1SylWord = sum(syll_count == 1),
              nPolySylWord = sum(syll_count > 2)
              )
```

```{r, cache = T}
train_stylometry <- train_stylometry %>% 
    left_join(train_tmp, by = 'id') %>%
    mutate(unique_r = nUniqueWord/word_count,
           w_p = word_count - punctuation_count,
           w_p_r = w_p/word_count,
           stop_r = nStopWord/word_count,
           w_p_stop = w_p - nStopWord,
           w_p_stop_r = w_p_stop/word_count,
           num_words_upper_r = uc_count/word_count,
           num_words_title_r = tc_count/word_count)

test_stylometry <- test_stylometry %>% 
    left_join(test_tmp, by = 'id') %>%
    mutate(unique_r = nUniqueWord/word_count,
           w_p = word_count - punctuation_count,
           w_p_r = w_p/word_count,
           stop_r = nStopWord/word_count,
           w_p_stop = w_p - nStopWord,
           w_p_stop_r = w_p_stop/word_count,
           num_words_upper_r = uc_count/word_count,
           num_words_title_r = tc_count/word_count)

```

```{r, cache = T}
train  %>%
    left_join(train_stylometry, by = 'id') %>%
    select(-c(text, id)) %>%
    filter(nchar < 1000) %>%
    group_by(author) %>%
    summarise_all(mean) %>%
    gather(feature, value, -author) %>%
    ggplot(aes(x = feature, y = value, color = author)) +
    scale_color_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
    geom_point(alpha = 0.6, size = 3) +
    scale_y_log10() +
    labs(x = 'feature', y = 'mean value by author') +
    coord_flip() +
    theme_bw()
```

Наиболее интересно выглядят признаки, связанные с прописными буквами.

## Признаки, основанные на тональности {.tabset .tabset-fade .tabset-pills}

### Тональность глаголов и интенсификаторы

Эта методика была найдена в одном из примеров и показалась интересной.

```{r, cache = T}
thought_verbs <- c('analyze', 'apprehend', 'assume', 'believe', 'calculate', 'cerebrate', 'cogitate',
                   'comprehend', 'conceive', 'concentrate', 'conceptualize', 'conclude', 'consider',
                   'construe', 'contemplate', 'deduce', 'deem', 'delibrate', 'desire', 'diagnose',
                   'doubt', 'envisage', 'envision', 'evaluate', 'excogitate', 'extrapolate', 'fantasize',
                   'forget', 'forgive', 'formulate', 'hate', 'hypothesize', 'imagine', 'infer', 
                   'intellectualize', 'intrigue', 'guess', 'introspect', 'judge', 'know', 'love', 
                   'lucubrate', 'marvel', 'meditate', 'note', 'notice', 'opine', 'perpend', 'philosophize',
                   'ponder', 'question', 'ratiocinate', 'rationalize', 'realize', 'reason', 'recollect', 
                   'reflect', 'remember', 'reminisce', 'retrospect', 'ruminate', 'sense', 'speculate',
                   'stew', 'strategize', 'suppose', 'suspect', 'syllogize', 'theorize', 'think', 
                   'understand', 'visualize', 'want', 'weigh', 'wonder')

loud_verbs <- c('cry', 'exclaim', 'shout', 'roar', 'scream', 'shriek', 'vociferated', 'bawl',
                'call', 'ejaculate', 'retort', 'proclaim', 'announce', 'protest', 'accost', 'declare')

neutral_verbs <- c('say', 'reply', 'observe', 'rejoin', 'ask', 'answer', 'return', 'repeat', 'remark',
                   'enquire', 'respond', 'suggest', 'explain', 'utter', 'mention')

quiet_verbs <- c('whisper', 'murmur', 'sigh', 'grumble', 'mumble', 'mutter', 'whimper', 'hush', 'falter',
                 'stammer', 'tremble', 'gasp', 'shudder')

qualifiers <- c('very', 'too', 'so', 'quite', 'rather', 'little', 'pretty', 'somewhat', 'various', 'almost', 
                'much', 'just', 'indeed', 'still', 'even', 'a lot', 'kind of', 'sort of')

train_tmp <- train %>%
    unnest_tokens(term, text, token = 'ngrams', n=1) %>% 
    bind_rows(train %>% unnest_tokens(term, text, token = 'ngrams', n=2)) %>%
    mutate(term = lemmatize_words(term),
           qualifier_ind = as.integer(term %in% qualifiers),
           thought_verbs_ind = as.integer(term %in% thought_verbs),
           loud_verbs_ind = as.integer(term %in% loud_verbs),
           neutral_verbs_ind = as.integer(term %in% neutral_verbs),
           quiet_verbs_ind = as.integer(term %in% quiet_verbs)) %>%
    group_by(id) %>%
    summarise(qualifier_count = sum(qualifier_ind),
              thought_verbs_count = sum(thought_verbs_ind),
              loud_verbs_count = sum(loud_verbs_ind),
              neutral_verbs_count = sum(neutral_verbs_ind),
              quiet_verbs_count = sum(quiet_verbs_ind))

test_tmp <- test %>%
    unnest_tokens(term, text, token = 'ngrams', n=1) %>% 
    bind_rows(test %>% unnest_tokens(term, text, token = 'ngrams', n=2)) %>%
    mutate(term = lemmatize_words(term),
           qualifier_ind = as.integer(term %in% qualifiers),
           thought_verbs_ind = as.integer(term %in% thought_verbs),
           loud_verbs_ind = as.integer(term %in% loud_verbs),
           neutral_verbs_ind = as.integer(term %in% neutral_verbs),
           quiet_verbs_ind = as.integer(term %in% quiet_verbs)) %>%
    group_by(id) %>%
    summarise(qualifier_count = sum(qualifier_ind),
              thought_verbs_count = sum(thought_verbs_ind),
              loud_verbs_count = sum(loud_verbs_ind),
              neutral_verbs_count = sum(neutral_verbs_ind),
              quiet_verbs_count = sum(quiet_verbs_ind))
 
train_stylometry <- train_stylometry %>% 
    left_join(train_tmp, by = 'id')

test_stylometry <- test_stylometry %>% 
    left_join(test_tmp, by = 'id')
```

```{r, cache = T}
train  %>%
    left_join(train_tmp, by = 'id') %>%
    select(-c(text, id)) %>%
    group_by(author) %>%
    summarise_all(mean) %>%
    gather(feature, value, -author) %>%
    ggplot(aes(x = feature, y = value, color = author)) +
    scale_color_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
    geom_point(alpha = 0.6, size = 3) +
    scale_y_log10() +
    labs(x = 'feature', y = 'mean value by author') +
    coord_flip() +
    theme_bw()
```

Разделение наблюдается по 4-м признакам из 5.

### Sentiment analysis

Для анализа эмоциональной окраски используется *tidytext*. Это не самый лучший пакет для анализа литературного текста, но он простой и покрывает большую часть nlp задач. В качестве словарей используются [AFINN](https://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010) и [NRC](https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm)

```{r, cache = T}
train_senti <- train %>%
    unnest_tokens(word, text) %>%
    inner_join(get_sentiments('nrc'), by = 'word') %>%
    count(id, sentiment) %>%
    spread(sentiment, n, sep = '_', fill = 0)

test_senti <- test %>%
    unnest_tokens(word, text) %>%
    inner_join(get_sentiments('nrc'), by = 'word') %>%
    count(id, sentiment) %>%
    spread(sentiment, n, sep = '_', fill = 0)

afinn_sentiment <- train %>%
    select(-author) %>%
    bind_rows(test) %>%
    unnest_tokens(word, text) %>%
    inner_join(get_sentiments('afinn'), by = 'word') %>%
    group_by(id) %>%
    summarise(sentiment_afinn = mean(value))

train_senti <- train_senti %>%
    left_join(afinn_sentiment, by = 'id') %>%
    replace_na(list(sentiment_afinn = 0))

test_senti <- test_senti %>%
    left_join(afinn_sentiment, by = 'id') %>%
    replace_na(list(sentiment_afinn = 0))

```

```{r, cache = T}
train %>%
    right_join(train_senti, by = 'id') %>%
    select(-c(text, id)) %>%
    group_by(author) %>%
    summarise_all(mean) %>%
    gather(feature, value, -author) %>%
    ggplot(aes(x = feature, y = value, color = author)) +
    scale_color_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
    geom_point(alpha = 0.6, size = 3) +
    labs(x = 'feature', y = 'mean value by author') +
    coord_flip() +
    theme_bw()
```

Интересным получится показатель *afinn*, хорошо сегментирующий тексты Лавкрафта, как несущие негативную окраску.

## Признаки, основанные на частях речи (POS-tagging) {.tabset .tabset-fade .tabset-pills}

В R присутствует большой выбор инструментов для разметки частей речи (а если хочется еще большего, всегда есть *reticulate*). Здесь используется пакет *udpipe*. Операция разметки медленная, поэтому отдельными блоками.

```{r, cache = T}
train <- train %>% 
    mutate(clean_text = str_replace(text, '^, ', ''),
           clean_text = str_replace(text, '^\\.\" ', ''),
           clean_text = str_replace_all(text, '\"', ''))

test <- test %>% 
    mutate(clean_text = str_replace_all(text, '\"', ''))
```

```{r, cache = T, message = F}
udmodel <- udpipe_download_model(language = 'english')
udmodel <- udpipe_load_model(file = udmodel$file_model)
```

```{r, cache = T}
x <- udpipe_annotate(object = udmodel, x = train$clean_text,  doc_id = train$id)
x <- as.data.frame(x)
```

```{r, cache = T}
y <- udpipe_annotate(object = udmodel, x = test$clean_text,  doc_id = test$id)
y <- as.data.frame(y)
```

```{r, cache = T}
train_udp_pos_count <- x %>%
    count(doc_id, xpos) %>%
    spread(xpos, n, fill = 0) %>%
    rename(id = doc_id)

test_udp_pos_count <- y %>%
    count(doc_id, xpos) %>%
    spread(xpos, n, fill = 0) %>% 
    select(-c(`''`, `,`, `.`, `:`, '``')) %>%
    rename(id = doc_id)

train_udp_pos_count <- train_udp_pos_count %>% 
    select(intersect(colnames(.), colnames(test_udp_pos_count)))

train_udp_upos_count <- x %>%
    count(doc_id, upos) %>%
    spread(upos, n, fill = 0) %>% 
    select(-PUNCT) %>%
    rename(id = doc_id)

test_udp_upos_count <- y %>%
    count(doc_id, upos) %>%
    spread(upos, n, fill = 0) %>% 
    select(-PUNCT) %>%
    rename(id = doc_id)

train_pos_tagged <- x
test_pos_tagged <- y
```

### Language-specific POS tag

```{r, cache = T}
train %>%  
    left_join(train_udp_pos_count, by = 'id') %>%
    select(-c(text, id, clean_text)) %>%
    group_by(author) %>%
    summarise_all(mean) %>%
    gather(feature, value, -author) %>%
    ggplot(aes(x = feature, y = value, color = author)) +
    scale_color_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
    geom_point(alpha = 0.6, size = 3) +
    labs(x = 'feature', y = 'mean value by author') +
    coord_flip() +
    theme_bw()
```

### Universal POS tags

```{r, cache = T}
train %>%  
    left_join(train_udp_upos_count, by = 'id') %>%
    select(-c(text, id, clean_text)) %>%
    group_by(author) %>%
    summarise_all(mean) %>%
    gather(feature, value, -author) %>%
    ggplot(aes(x = feature, y = value, color = author)) +
    scale_color_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
    geom_point(alpha = 0.6, size = 3) +
    labs(x = 'feature', y = 'mean value by author') +
    coord_flip() +
    theme_bw()
```

## Признаки, основанные на n-граммах

```{r, cache = T}
author_unigrams_tfidf <- train %>%
    unnest_tokens(term, text, token = 'ngrams', n=1) %>%
    count(author, term) %>%
    bind_tf_idf(term, author, n)

author_bigrams_tfidf <- train %>%
    unnest_tokens(term, text, token = 'ngrams', n=2) %>%
    count(author, term) %>%
    bind_tf_idf(term, author, n)

author_trigrams_tfidf <- train %>%
    unnest_tokens(term, text, token = 'ngrams', n=3) %>%
    count(author, term) %>%
    bind_tf_idf(term, author, n)

author_tetragrams_tfidf <- train %>%
    unnest_tokens(term, text, token = 'ngrams', n=4) %>%
    count(author, term) %>%
    bind_tf_idf(term, author, n)

author_char_bigrams_tfidf <- train %>% 
    unnest_tokens(shingle, text, token = 'character_shingles', n=2, strip_non_alphanum = F) %>%
    count(author, shingle) %>%
    bind_tf_idf(shingle, author, n)

author_char_trigrams_tfidf <- train %>% 
    unnest_tokens(shingle, text, token = 'character_shingles', n=3, strip_non_alphanum = F) %>%
    count(author, shingle) %>%
    bind_tf_idf(shingle, author, n)

author_char_tetragrams_tfidf <- train %>% 
    unnest_tokens(shingle, text, token = 'character_shingles', n=4, strip_non_alphanum = F) %>%
    count(author, shingle) %>%
    bind_tf_idf(shingle, author, n)

author_char_pentagrams_tfidf <- train %>% 
    unnest_tokens(shingle, text, token = 'character_shingles', n=5, strip_non_alphanum = F) %>%
    count(author, shingle) %>%
    bind_tf_idf(shingle, author, n)
```

```{r, cache = T}
get_author_ngrams <- function(df, author_id){
    df %>% filter(idf == max(idf, na.rm = T) & author == author_id & n > 25) %>% .[,2]
}

df_ngram <- list(author_unigrams_tfidf, author_bigrams_tfidf, author_trigrams_tfidf, author_tetragrams_tfidf,
                author_char_bigrams_tfidf, author_char_trigrams_tfidf, author_char_tetragrams_tfidf,
                author_char_pentagrams_tfidf)

eap <- mapply(get_author_ngrams, df_ngram, rep('EAP', 8)) %>% unlist() %>% unique()
hpl <- mapply(get_author_ngrams, df_ngram, rep('HPL', 8)) %>% unlist() %>% unique()
mws <- mapply(get_author_ngrams, df_ngram, rep('MWS', 8)) %>% unlist() %>% unique()

get_total_ngrams <- function(df){
    map_df(1:4, ~ unnest_tokens(df, term, text, token = 'ngrams', n = .x)) %>%
    bind_rows(map_df(2:5, ~ unnest_tokens(df, term, text, token = 'character_shingles', n = .x,
                                          strip_non_alphanum = F))) %>%
    mutate(EAP_only_ind = as.integer(term %in% eap),
           HPL_only_ind = as.integer(term %in% hpl),
           MWS_only_ind = as.integer(term %in% mws)) %>%
    group_by(id) %>%
    summarise(EAP_only_count = sum(EAP_only_ind),
              HPL_only_count = sum(HPL_only_ind),
              MWS_only_count = sum(MWS_only_ind))
}

train_author_only <- get_total_ngrams(train)
test_author_only <- get_total_ngrams(test)

get_author_pair_ngrams <- function(df, author_id1, author_id2){
    df %>% filter(author == author_id1 | author == author_id2, idf == log(1.5)) %>%
        group_by_at(2) %>% 
        count(wt = n) %>% filter(n > 50) %>% .[,1]
}

eap_hpl <- mapply(get_author_pair_ngrams, df_ngram, rep('EAP', 8), rep('HPL', 8)) %>% unlist() %>% unique()
eap_mws <- mapply(get_author_pair_ngrams, df_ngram, rep('EAP', 8), rep('MWS', 8)) %>% unlist() %>% unique()
hpl_mws <- mapply(get_author_pair_ngrams, df_ngram, rep('HPL', 8), rep('MWS', 8)) %>% unlist() %>% unique()

get_total_pair_ngrams <- function(df){
    map_df(1:4, ~ unnest_tokens(df, term, text, token = 'ngrams', n = .x)) %>%
    bind_rows(map_df(2:5, ~ unnest_tokens(df, term, text, token = 'character_shingles', n = .x,
                                          strip_non_alphanum = F))) %>%
    mutate(EAP_HPL_only_ind = as.integer(term %in% eap_hpl),
           EAP_MWS_only_ind = as.integer(term %in% eap_mws),
           HPL_MWS_only_ind = as.integer(term %in% hpl_mws)) %>%
    group_by(id) %>%
    summarise(EAP_HPL_only_count = sum(EAP_HPL_only_ind),
              EAP_MWS_only_count = sum(EAP_MWS_only_ind),
              HPL_MWS_only_count = sum(HPL_MWS_only_ind))
}

train_author_pair_only <- get_total_pair_ngrams(train)
test_author_pair_only <- get_total_pair_ngrams(test)
```

```{r, cache = T}
train %>% 
    left_join(train_author_only, by = 'id') %>%
    left_join(train_author_pair_only, by = 'id') %>%
    select(-c(text, id, clean_text)) %>%
    group_by(author) %>%
    summarise_all(mean) %>%
    gather(feature, value, -author) %>%
    ggplot(aes(x = feature, y = value, color = author)) +
    scale_color_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
    geom_point(alpha = 0.6, size = 3) +
    labs(x = 'feature', y = 'mean value by author') +
    coord_flip() +
    theme_bw()


```

Признаки демонстрируют выраженную разделительную способность.

## Гендерные признаки

Проверим, нет ли у авторов предпочтений в области полов.

```{r, cache = T}
get_gender_plot <- function(m, f){
    train %>%
        unnest_tokens(word, text) %>%
        filter((word == m) | (word == f)) %>%
        mutate(word = as.factor(word)) %>%
        mutate(word = fct_relevel(word, f, m)) %>%
        ggplot(aes(word, fill = author)) +
        geom_bar(position = 'dodge') +
        scale_fill_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
        theme_bw()
}

pl1 <- get_gender_plot('man', 'woman')
pl2 <- get_gender_plot('he', 'she')
pl3 <- get_gender_plot('him', 'her')

pl4 <- train %>%
    unnest_tokens(word, text) %>%
    mutate(male = (word %in% c('he', 'him', 'his', 'male', 'man', 'gentleman', 'sir', 'lord', 'men'))) %>%
    mutate(female = (word %in% c('she', 'her', 'hers', 'female', 'woman', 'lady', 'madam', 'women' ))) %>%
    unite(sex, male, female) %>%
    mutate(sex = fct_recode(as.factor(sex), male = 'TRUE_FALSE', 
                          female = 'FALSE_TRUE', other = 'FALSE_FALSE')) %>%
    filter(sex != 'other') %>%
    ggplot(aes(sex, fill = author)) +
    labs(x = 'gender indicators') +
    geom_bar(position = 'dodge') +
    scale_fill_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
    theme_bw()

grid.arrange(pl1, pl2, pl3, pl4, nrow = 2)

train_gender <- train %>%
    unnest_tokens(word, text) %>% 
    mutate(male = (word %in% c('he', 'him', 'his', 'male', 'man', 'gentleman', 'sir', 'lord', 'men'))) %>%
    mutate(female = (word %in% c('she', 'her', 'hers', 'female', 'woman', 'lady', 'madam', 'women' ))) %>% 
    group_by(id) %>%
    summarise(male = sum(male), female = sum(female))

test_gender <- test %>%
    unnest_tokens(word, text) %>% 
    mutate(male = (word %in% c('he', 'him', 'his', 'male', 'man', 'gentleman', 'sir', 'lord', 'men'))) %>%
    mutate(female = (word %in% c('she', 'her', 'hers', 'female', 'woman', 'lady', 'madam', 'women' ))) %>% 
    group_by(id) %>%
    summarise(male = sum(male), female = sum(female))
```

Лавкрафта, видимо, не особенно интересовали женские пресонажи. Хорошо, что он жил и работал не в наше время. По в общем-то тоже на грани фола. Шелли, ожидаемо, более склонна к женским персонажам. Тем не менее, у всех троих мужской пол доминирует в произведениях.

## Аллитерации и ассонансы

Аллитерация — повторение одинаковых или однородных согласных, ассонанс - повторение гласных. Используется в основном в поэзии для придания особого звучания тексту. Для простоты примем аллитерацию за повторение первых букв в словах, идущих подряд. *[a]rab [a]bdul [a]lhazred*. Также для большей надежности удалим стоп слова перед проверкой.

```{r, cache = T}
get_alliterations <- function(df, stop = F){
    if(stop){
        df <- df %>%
            select(id, text) %>% 
            unnest_tokens(word, text) %>% 
            anti_join(stop_words, by = 'word')
    }else{
        df <- df %>%
            select(id, text) %>% 
            unnest_tokens(word, text)
    }

    df %>% 
        mutate(first = str_sub(word, start = 1, end = 1),
             f_lead_1 = lead(first, n = 1),
             f_lead_2 = lead(first, n = 2),
             id_lead_1 = lead(id, n = 1),
             id_lead_2 = lead(id, n = 2),
             allit2 = first == f_lead_1 & id == id_lead_1,
             allit3 = first == f_lead_1 & first == f_lead_2 & id == id_lead_1 & id == id_lead_2
             ) %>%
        filter(!is.na(allit2)) %>%
        group_by(id, allit2) %>%
        count() %>%
        spread(allit2, n) %>%
        mutate(has_allit = ifelse(!is.na(`TRUE`), 1, 0)) %>%
        select(id, has_allit)
}

train_alliterations <- get_alliterations(train, stop = T)
test_alliterations <- get_alliterations(test, stop = T)

get_allit_plot <- function(df, title){
    df %>% 
        left_join(train, by = 'id') %>% 
        group_by(author, has_allit) %>% 
        count() %>% 
        ungroup() %>% 
        (function(df) left_join(df, df %>% group_by(author) %>% summarise(s = sum(n)), by = 'author')) %>% 
        mutate(n = (n/s) * 100) %>% 
        filter(has_allit == T) %>% 
        ggplot(aes(x = author, y = n, fill = author)) +
        geom_col() +
        scale_fill_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
        labs(y = "fraction of sentences with alliterations") +
        ggtitle(title) +
        theme_bw()
}

pl1 <- get_allit_plot(get_alliterations(train, stop = T), title = 'without stop words')
pl2 <- get_allit_plot(get_alliterations(train, stop = F), title = 'with stop words')

grid.arrange(pl1, pl2, nrow = 1)
```

Лавкрафт незначительно опережает остальных в поэтичности речи.

# Моделирование

## Оценка качества модели

Для оценки качества модели *Kaggle* предлагает использовать [**multi-class logarithmic loss**](https://www.kaggle.com/c/spooky-author-identification/overview/evaluation). В **R** эту метрику можно взять  из пакета *MLmetrics*, но следует отметить, что в случае подачи ей на вход датафрейма в некоторых случаях можно получить на выходе ***NA***. Поэтому имеет смысл немного переписать функцию для таких случаев.

\begin{align}
    logloss = -\frac{1}{N}{\sum_{i=1}^N\sum_{j=1}^M y_{ij}log(p_{ij})}
\end{align}

Здесь $N$ - число наблюдений, $M$ - число классов, $y_{ij}$ = 1, если наблюдение $i$ принадлежит классу $j$, иначе 0, $p_{ij}$ - предсказанные вероятности.

```{r, cache = T}
MultiLogLoss <- function(y_pred, y_true) {
    if (is.matrix(y_true) == FALSE) {
        y_true <- model.matrix(~ 0 + ., data.frame(as.character(y_true)))
    }
    y_pred <- as.matrix(y_pred)
    eps <- 1e-15
    N <- nrow(y_pred)
    y_pred <- pmax(pmin(y_pred, 1 - eps), eps)
    MultiLogLoss <- (-1 / N) * sum(y_true * log(y_pred))
    return(MultiLogLoss)
}
```

Поскольку работа построена вокруг соревнования на *Kaggle*, оценка модели на тестовых данных будет производиться непосредственно через саму платформу [sunmission](https://www.kaggle.com/c/spooky-author-identification/submit). Для тренировочных данных оценка будет производиться непосредственно по получению результата.

## Используемые модели

При построения моделей будут использоваться как найденные выше признаки, так и векторные представления слов. В качестве baseline модели будет использоваться *Наивный байесовский классификатор*.

Далее будет рассмотрено несколько простых моделей на нейронных сетях с разными входными данными для сбора предсказаний и рекуррентная нейросеть на подготовленных заранее эмбеддингах.

Предсказания предыдущих моделей составят основу для ансамбля, моделировать который будет использован *xgboost*.

Одной из задач данной работы стоит попадание в топ 20%, что эквивалентно 248 строчке рейтинга или оценке в **0.32237**. По достижению этого показателя, основной этап работы будет считаться выполненным.

```{r, cache = T}
train_total <- train %>% 
    select(-c(text, clean_text)) %>% 
    left_join(train_stylometry, by = 'id') %>% 
    left_join(train_senti, by = 'id') %>% 
    left_join(train_udp_pos_count, by = 'id') %>% 
    left_join(train_udp_upos_count, by = 'id') %>% 
    left_join(train_gender, by = 'id') %>% 
    left_join(train_author_only, by = 'id') %>% 
    left_join(train_author_pair_only, by = 'id') %>% 
    left_join(train_alliterations, by = 'id') %>% 
    select(-SYM)
train_total[is.na(train_total)] <- 0

test_total <- test %>% 
    select(-c(text, clean_text)) %>% 
    left_join(test_stylometry, by = 'id') %>% 
    left_join(test_senti, by = 'id') %>% 
    left_join(test_udp_pos_count, by = 'id') %>% 
    left_join(test_udp_upos_count, by = 'id') %>% 
    left_join(test_gender, by = 'id') %>% 
    left_join(test_author_only, by = 'id') %>% 
    left_join(test_author_pair_only, by = 'id') %>% 
    left_join(test_alliterations, by = 'id')
test_total[is.na(test_total)] <- 0
```

## Наивный байесовский классификатор

[Наивный байесовский классификатор](https://en.wikipedia.org/wiki/Naive_Bayes_classifier). Это простая модель, активно используемая для начала работы за счет своего быстродействия и, практически, отсутствующих параметров для настройки. В **R** она реализована во многих пакетах, в частности, *e1071*, *quanteda*, *klaR*. В данной работе используется специализированный пакет *fastNaiveBayes*, модель из которого работает быстрее остальных на больших объемах данных.

```{r, cache = T}
set.seed(42)
folds <- createFolds(train$author, k = 5)
y_train <- to_categorical(as.integer(as.factor(train$author)), num_classes = NULL)
y_train <- y_train[,2:4]
```

```{r, cache = T, message = F, warning = F}
df <- train %>%
    select(-c(author)) %>% 
    unnest_tokens(token, text, token = 'words', to_lower = F) %>% 
    select(-id) %>%
    count(token) %>%
    .$token
    
df_cnt <- train %>%
    select(-c(author)) %>%
    bind_rows(test) %>%
    unnest_tokens(token, text, token = 'words', to_lower = F) %>%
    count(id, token) %>%
    filter(token %in% df) %>%
    cast_dtm(id, token, n)
    # cast_dtm(id, token, n, weighting = tm::weightTfIdf)

x_train_df <-  df_cnt[train$id,] %>% as.matrix
x_test_df <- df_cnt[test$id,] %>% as.matrix
rm(df, df_cnt)
    
model_fnb <- fnb.train(x_train_df, train$author, laplace = 1)
# MultiLogLoss(y_pred = predict(model_fnb, x_train_df, type = 'raw'), y_true = y_train)

fnb <- predict(model_fnb, x_test_df, type = 'raw')
read_csv(file = 'data/sample_submission.csv') %>% 
    select(id) %>% 
    bind_cols(., fnb %>% as_tibble()) %>% 
    write_csv('sub_fnb.csv')
```

На тренировочных данных оценка MultiLogLoss составляет **`r MultiLogLoss(y_pred = predict(model_fnb, x_train_df, type = 'raw'), y_true = y_train)`**. Для тестового варианта Kaggle привел оценку в **0.47287**. Это далеко не лучший показатель, эквивалентный 764 позиции в рейтинге из 1243. Тем не менее, для первой модели, результат стоящий. Напомним, что пороговый показатель **0.32237**.


## Стек {.tabset .tabset-fade .tabset-pills}

```{r, cache = T}
get_ngram_df <- function(in_token, in_n, in_cnt){
    df <- train %>%
        select(-author) %>% 
        unnest_tokens(token, text, token = in_token, n = in_n, to_lower = F) %>% 
        select(-id) %>%
        count(token) %>%
        filter(n > in_cnt) %>%
        .$token
    
    df_cnt <- train %>%
        select(-author) %>%
        bind_rows(test) %>%
        unnest_tokens(token, text, token = in_token, n = in_n, to_lower = F) %>%
        count(id, token) %>%
        filter(token %in% df) %>%
        cast_dtm(id, token, n)
    
    if(in_cnt != 0){
        rowsRemoved <- setdiff(c(train$id,test$id),rownames(df_cnt))
        allZeros <- matrix(0, length(rowsRemoved), ncol(df_cnt), 
                           dimnames = list(rowsRemoved, colnames(df_cnt)))
        df_cnt <- df_cnt %>% rbind(allZeros)
        rm(rowsRemoved,allZeros)
    }
    
    x_train_df <- df_cnt[train$id,] %>% as.matrix
    x_test_df <- df_cnt[test$id,] %>% as.matrix
    rm(df, df_cnt)
        
    return(list(x_train_df, x_test_df))
}

get_ngram_model <- function(df_train, df_test, fold, y_train, filepath){
    nn_model <- keras_model_sequential() %>%
        layer_dense(units = 16, activation = 'relu', input_shape = ncol(df_train)) %>% 
        layer_dense(units = 16, activation = 'relu') %>%
        layer_dense(units = 3, activation = 'softmax')
    
    nn_model %>% compile(
        loss = 'categorical_crossentropy',
        optimizer = 'rmsprop',
        metrics = c('accuracy')
    )
    
    frozen_nn_model <- nn_model %>%
        fit(
            df_train[-fold,], y_train[-fold,],
            batch_size = 2^9,
            epochs = 20,
            validation_split = 0.1, 
            verbose = F,
            callbacks = list(
                callback_early_stopping(monitor = 'val_loss', patience = 2),
                callback_model_checkpoint(
                    filepath = paste0(filepath, '.hdf5'),
                    monitor = 'val_loss',
                    mode = 'min',
                    save_best_only = T)
          )
    )
    nn_model <- load_model_hdf5(paste0(filepath, '.hdf5'))
    
    train_pred <- nn_model %>%
        predict(df_train[fold,])
    test_pred <- nn_model %>%
        predict(df_test)
    fold_eval <- nn_model %>%
        evaluate(df_train[fold,], y_train[fold,])
    out <- list(train_pred = train_pred, test_pred = test_pred, logloss = fold_eval$loss, acc = fold_eval$acc )
    k_clear_session()
    return(out)
}

get_ngram_predictions <- function(ngram_df, filepath){
    train_count <- matrix(0, nrow = nrow(train), ncol = 3)
    test_count <- matrix(0, nrow = nrow(test), ncol = 3)
    metrics_count <- matrix(0, 5, 2)

    for(i in 1:5){
        results_count <- get_ngram_model(ngram_df[[1]], ngram_df[[2]], folds[[i]], y_train, filepath)
        train_count[folds[[i]], ] <- results_count$train_pred
        test_count <- test_count + (results_count$test_pred)/5
        metrics_count[i,1] <- results_count$logloss
        metrics_count[i,2] <- results_count$acc
        gc()
    }   
  
    train_count <- train_count %>%
        as.data.frame()
    test_count <- test_count %>%
        as.data.frame()
    metrics_count <- metrics_count %>%
        as.data.frame()
    rownames(metrics_count) <- paste0("fold ", 1:5, ":")
    return(list(train_count, test_count, metrics_count))
}
```

### Униграммы

```{r, message = F, warning = F, cache = T}
ngram_df <- get_ngram_df(in_token = 'ngrams', in_n = 1, in_cnt = 0)
ngram_predictions <- get_ngram_predictions(ngram_df, 'word_1g_count')
train_word_1g_count <- ngram_predictions[[1]] %>%  
    rename(word_1g_count_EAP=V1, word_1g_count_HPL=V2, word_1g_count_MWS=V3)
test_word_1g_count <- ngram_predictions[[2]] %>% 
    rename(word_1g_count_EAP=V1, word_1g_count_HPL=V2, word_1g_count_MWS=V3)
metrics_word_1g_count <- ngram_predictions[[3]] %>% 
    rename(logloss=V1, acc=V2)
rm(ngram_df, ngram_predictions)
invisible(gc())
metrics_word_1g_count
```

### Биграммы

```{r, message = F, warning = F, cache = T}
ngram_df <- get_ngram_df(in_token = 'ngrams', in_n = 2, in_cnt = 2)
ngram_predictions <- get_ngram_predictions(ngram_df, 'word_2g_count')
train_word_2g_count <- ngram_predictions[[1]] %>%  
    rename(word_2g_count_EAP=V1, word_2g_count_HPL=V2, word_2g_count_MWS=V3)
test_word_2g_count <- ngram_predictions[[2]] %>% 
    rename(word_2g_count_EAP=V1, word_2g_count_HPL=V2, word_2g_count_MWS=V3)
metrics_word_2g_count <- ngram_predictions[[3]] %>% 
    rename(logloss=V1, acc=V2)
rm(ngram_df, ngram_predictions)
invisible(gc())
metrics_word_2g_count
```

### Триграммы

```{r, message = F, warning = F, cache = T}
ngram_df <- get_ngram_df(in_token = 'ngrams', in_n = 3, in_cnt = 2)
ngram_predictions <- get_ngram_predictions(ngram_df, 'word_3g_count')
train_word_3g_count <- ngram_predictions[[1]] %>%  
    rename(word_3g_count_EAP=V1, word_3g_count_HPL=V2, word_3g_count_MWS=V3)
test_word_3g_count <- ngram_predictions[[2]] %>% 
    rename(word_3g_count_EAP=V1, word_3g_count_HPL=V2, word_3g_count_MWS=V3)
metrics_word_3g_count <- ngram_predictions[[3]] %>% 
    rename(logloss=V1, acc=V2)
rm(ngram_df, ngram_predictions)
invisible(gc())
metrics_word_3g_count
```

### Тетраграммы

```{r, message = F, warning = F, cache = T}
ngram_df <- get_ngram_df(in_token = 'ngrams', in_n = 4, in_cnt = 2)
ngram_predictions <- get_ngram_predictions(ngram_df, 'word_4g_count')
train_word_4g_count <- ngram_predictions[[1]] %>%  
    rename(word_4g_count_EAP=V1, word_4g_count_HPL=V2, word_4g_count_MWS=V3)
test_word_4g_count <- ngram_predictions[[2]] %>% 
    rename(word_4g_count_EAP=V1, word_4g_count_HPL=V2, word_4g_count_MWS=V3)
metrics_word_4g_count <- ngram_predictions[[3]] %>% 
    rename(logloss=V1, acc=V2)
rm(ngram_df, ngram_predictions)
invisible(gc())
metrics_word_4g_count
```


## Нейронные сети с эмбеддингами

```{r, message = F, warning = F, cache = T}

max_features <- 30000

tokenizer <- text_tokenizer(num_words = max_features, lower = T) %>%
    fit_text_tokenizer(train$text)  
tokenizer$document_count
train_seq <- texts_to_sequences(tokenizer, train$text)
test_seq <- texts_to_sequences(tokenizer, test$text)
max_len <- 100
train_seq_padded <- pad_sequences(train_seq, maxlen = max_len)
test_seq_padded <- pad_sequences(test_seq, maxlen = max_len)

model <- keras_model_sequential() %>% 
    layer_embedding(input_dim = max_features, output_dim = 64, input_length = max_len) %>%
    bidirectional(layer_lstm(units = 50, recurrent_dropout = 0.2, return_sequences = T)) %>%
    layer_global_max_pooling_1d() %>% 
    layer_dropout(rate = 0.2) %>% 
    layer_dense(units = 3, activation = 'softmax') %>% 
    compile(
        loss = "binary_crossentropy",
        optimizer = "rmsprop",
        metrics = "accuracy"
        )
    
bilstm <- model %>%
    fit(
      train_seq_padded[-folds[[1]],], y_train[-folds[[1]],],
      batch_size = 2^9, 
      epochs = 20,
      validation_split = 0.1,
      verbose = T,
      callbacks = list(
          callback_early_stopping(monitor = "val_loss", patience = 2),
          callback_model_checkpoint(
              filepath = "word_vec_bilstm.hdf5",
              monitor = "val_loss",
              save_best_only = TRUE)
          )
      )

bilstm_m <- load_model_hdf5("word_vec_bilstm.hdf5")

test_pred_bilstm <- bilstm_m %>% predict(test_seq_padded)
fold_eval <- bilstm_m %>% evaluate(train_seq_padded[folds[[i]],], y_train[folds[[i]],])

read_csv(file = 'data/sample_submission.csv') %>% 
    select(id) %>% 
    bind_cols(., test_pred_bilstm %>% as_tibble()) %>% 
    rename(EAP = V1, HPL = V2, MWS = V3) %>% 
    write_csv('sub_bilstm.csv')



hist <- model %>%
    fit(
        x_train,
        y_train,
        batch_size = batch_size,
        epochs = epochs,
        validation_split = 0.3
  )
plot(hist)

model %>% 
  layer_embedding(input_dim = vocab_size, output_dim = 256) %>%
  bidirectional(layer_lstm(units = 128)) %>%
  layer_dropout(rate = 0.5) %>% 
  layer_dense(units = 1, activation = 'sigmoid') # 1 = Hadley, 0 = Marie

model %>% 
  compile(optimizer = "rmsprop",
          loss = "binary_crossentropy",
          metrics = c("accuracy"))

history <- model %>%  # fit the model (this will take a while...)
  fit(x_train, 
      y_train, 
      epochs = 25, 
      batch_size = nrow(train_data)/5, 
      validation_split = 1/5)


```








```{r eval=FALSE, include=FALSE}
library(doFuture)
registerDoFuture()
plan(multiprocess, workers = availableCores() - 1)

set.seed(42)
train_df <- train_total %>% 
    select(-id)

colnames(train_df) <- make.names(colnames(train_df))

ctrl <- trainControl(method = "cv", number = 10, selectionFunction = "best", allowParallel = T, classProbs = T, summaryFunction=mnLogLoss)

fit.rpart2 <- train(author ~ ., 
                   data = train_df, 
                   method = "ranger", 
                   trControl = ctrl,
                   importance = "permutation",
                   metric="logLoss",
                   num.trees = 50,
                   num.threads = 10)

print(fit.rpart2)

results2 <- fit.rpart2$resample %>% 
           select(-Resample) %>% 
           mutate(Models = "rf")
```


```{r eval=FALSE, include=FALSE}
train_all_sp <- train %>% select(-c(text, clean_text)) %>% 
    bind_cols(train_word_1g_count) %>%
    bind_cols(train_word_2g_count) %>%
    bind_cols(train_word_3g_count) %>%
    bind_cols(train_word_4g_count)

test_all_sp <- test %>% select(-c(text, clean_text)) %>% 
    bind_cols(test_word_1g_count) %>%
    bind_cols(test_word_2g_count) %>%
    bind_cols(test_word_3g_count) %>%
    bind_cols(test_word_4g_count)

sp_plot <- train_all_sp %>%
    select(-c(id, clean_text)) %>%
    group_by(author) %>%
    summarise_all(mean) %>%
    gather(feature, value, -author) %>%
    ggplot(aes(x = feature, y = value, color = author, fill = author, size = value)) +
    geom_point(alpha = 0.6) +
    labs(x = "Feature", y = "Mean Value by Author") +
    coord_flip()
ggplotly(sp_plot, tooltip = c("x","y","fill"))


# Consolidating all features for training and testing
dtrain <- train %>% select(-c(text, clean_text)) %>% 
    left_join(train_stylometry, by = 'id') %>% 
    left_join(train_senti, by = 'id') %>% 
    left_join(train_udp_pos_count, by = 'id') %>% 
    left_join(train_udp_upos_count, by = 'id') %>% 
    left_join(train_gender, by = 'id') %>% 
    left_join(train_author_only, by = 'id') %>% 
    left_join(train_author_pair_only, by = 'id') %>% 
    left_join(train_alliterations, by = 'id') %>% 
    left_join(train_all_sp, by = c('id', 'author')) %>% 
    select(-SYM)
dtrain[is.na(dtrain)] <- 0

dtest <- test %>% select(-c(text, clean_text)) %>% 
    left_join(test_stylometry, by = 'id') %>% 
    left_join(test_senti, by = 'id') %>% 
    left_join(test_udp_pos_count, by = 'id') %>% 
    left_join(test_udp_upos_count, by = 'id') %>% 
    left_join(test_gender, by = 'id') %>% 
    left_join(test_author_only, by = 'id') %>% 
    left_join(test_author_pair_only, by = 'id') %>% 
    left_join(test_alliterations, by = 'id') %>% 
    left_join(test_all_sp, by = 'id') 
dtest[is.na(dtest)] <- 0

dim(dtrain)

dim(dtest)

# Spearman correlations between covariates
sCorr <- cor(dtrain %>% select(-id, -author), method = "spearman")
sigCorr <- findCorrelation(sCorr, cutoff = .8, names = TRUE)
ggcorrplot(cor(dtrain %>% select(sigCorr), method = "spearman"))

# One-way analysis of variance 
anova <- apply(dtrain %>% select(-id,-author), 2,
               function(x, y)
               {
                   lm_model <- lm(x ~ y)
                   unlist(glance(lm_model)[c("r.squared", "p.value")])
               },
               y = dtrain$author)
anova <- as.data.frame(t(anova))
names(anova) <- c("r.squared", "p.value")
anova$p.adj <- p.adjust(anova$p.value , method = "fdr")
anova$predictor <- rownames(anova)

anova_plot <- anova %>% mutate(grp = case_when(
    p.adj == 0 ~ "p.adj==0",
    p.adj > 0 & p.adj < 0.01 ~ "0<p.adj<0.01",
    p.adj >= 0.01 ~ "p.adj>=0.01")) %>%
    ggplot(aes(x=grp, y=r.squared, color=grp)) +
    geom_jitter(aes(text = (paste("Predictor:", predictor, 
                                  "<br>p.adj:", p.adj,
                                  "<br>R squared:", r.squared))), 
                alpha = 0.4, size = 3, height = 0) +
    labs(x = "Features grouped by", y = "Coefficient of determination") +
    theme(legend.position = "none")
ggplotly(anova_plot, tooltip = c("text"))

# Features having high correlations
(highCorrVars <- findCorrelation(sCorr, cutoff = .98, names = TRUE))

# Feature selection at 0.01 significance level
sig_.01_f <- anova %>% filter(p.adj < 0.01) %>% .$predictor

# Function for measuring accuracy and log loss
customSummary <- function(data, lev = levels(data$obs), model = NULL) {
    mcs <- multiClassSummary(data, lev = levels(data$obs), model = NULL)
    mnll <- mnLogLoss(data, lev = levels(data$obs), model = NULL)
    out <- c(mnll, mcs['Accuracy'])
}

# Xgboost model
set.seed(42)
sai_xgbt <- train(x = dtrain %>% select(-c(id, author)), #%>% select(setdiff(sig_.01_f, highCorrVars)),
                  y = factor(dtrain$author),
                  method = "xgbTree",
                  metric = "logLoss",
                  tuneGrid = expand.grid(nrounds = seq(50,1400,50),
                                         max_depth = 4, 
                                         eta = 0.02,
                                         gamma = 0.5,
                                         colsample_bytree = 0.35,
                                         min_child_weight = 4,
                                         subsample = 0.85),
                  trControl = trainControl(method = "cv",
                                           number = 10,
                                           classProbs = TRUE,
                                           summaryFunction = customSummary,
                                           search = "grid")
)
sai_xgbt

sub_sai_xgbt <- read_csv("data/sample_submission.csv") %>% select(id) %>%
    bind_cols(predict(sai_xgbt, dtest %>% select(-c(id)), type = "prob"))
write_excel_csv(sub_sai_xgbt, "stacked_xgb_short.csv")


```

```{r eval=FALSE, include=FALSE}
MAX_SEQUENCE_LENGTH <- 1000
MAX_NUM_WORDS <- 20000
EMBEDDING_DIM <- 100
VALIDATION_SPLIT <- 0.2

embeddings_index <- new.env(parent = emptyenv())
lines <- readLines(file.path('../', 'glove.840B.300d.txt'))
for (line in lines) {
  values <- strsplit(line, ' ', fixed = TRUE)[[1]]
  word <- values[[1]]
  coefs <- as.numeric(values[-1])
  embeddings_index[[word]] <- coefs
}

cat(sprintf('Found %s word vectors.\n', length(embeddings_index)))

texts <- train$text

# finally, vectorize the text samples into a 2D integer tensor
tokenizer <- text_tokenizer(num_words=MAX_NUM_WORDS)
tokenizer %>% fit_text_tokenizer(texts)

# save the tokenizer in case we want to use it again
# for prediction within another R session, see:
# https://keras.rstudio.com/reference/save_text_tokenizer.html
save_text_tokenizer(tokenizer, "tokenizer")

sequences <- texts_to_sequences(tokenizer, texts)

word_index <- tokenizer$word_index
cat(sprintf('Found %s unique tokens.\n', length(word_index)))

data <- pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
labels <- y_train

cat('Shape of data tensor: ', dim(data), '\n')
cat('Shape of label tensor: ', dim(labels), '\n')

# split the data into a training set and a validation set
indices <- 1:nrow(data)
indices <- sample(indices)
data <- data[indices,]
labels <- labels[indices,]
num_validation_samples <- as.integer(VALIDATION_SPLIT * nrow(data))

xx_train <- data[-(1:num_validation_samples),]
yy_train <- labels[-(1:num_validation_samples),]
x_val <- data[1:num_validation_samples,]
y_val <- labels[1:num_validation_samples,]

cat('Preparing embedding matrix.\n')

# prepare embedding matrix
num_words <- min(MAX_NUM_WORDS, length(word_index) + 1)
prepare_embedding_matrix <- function() {
  embedding_matrix <- matrix(0L, nrow = num_words, ncol = EMBEDDING_DIM)
  for (word in names(word_index)) {
    index <- word_index[[word]]
    if (index >= MAX_NUM_WORDS)
      next
    embedding_vector <- embeddings_index[[word]]
    if (!is.null(embedding_vector)) {
      # words not found in embedding index will be all-zeros.
      embedding_matrix[index,] <- embedding_vector
    }
  }
  embedding_matrix
}

embedding_matrix <- prepare_embedding_matrix()

# load pre-trained word embeddings into an Embedding layer
# note that we set trainable = False so as to keep the embeddings fixed
embedding_layer <- layer_embedding(
  input_dim = num_words,
  output_dim = EMBEDDING_DIM,
  weights = list(embedding_matrix),
  input_length = MAX_SEQUENCE_LENGTH,
  trainable = FALSE
)
                           
cat('Training model\n')

# train a 1D convnet with global maxpooling
sequence_input <- layer_input(shape = list(MAX_SEQUENCE_LENGTH), dtype='int32')

preds <- sequence_input %>%
  embedding_layer %>% 
  layer_conv_1d(filters = 128, kernel_size = 5, activation = 'relu') %>% 
  layer_max_pooling_1d(pool_size = 5) %>% 
  layer_conv_1d(filters = 128, kernel_size = 5, activation = 'relu') %>% 
  layer_max_pooling_1d(pool_size = 5) %>% 
  layer_conv_1d(filters = 128, kernel_size = 5, activation = 'relu') %>% 
  layer_max_pooling_1d(pool_size = 35) %>% 
  layer_flatten() %>% 
  layer_dense(units = 128, activation = 'relu') %>% 
  layer_dense(units = length(labels_index), activation = 'softmax')


model <- keras_model(sequence_input, preds)

model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = 'rmsprop',
  metrics = c('acc')  
)

model %>% fit(
  x_train, y_train,
  batch_size = 128,
  epochs = 10,
  validation_data = list(x_val, y_val)
)


```

<!-- \begin{equation} -->
<!--   \label{eq:1} -->
<!--   idf_i = \mbox{log} \frac{|D|}{|{d : t_i \in d}|} -->
<!-- \end{equation} -->

<!-- \begin{align} -->
<!--   \tag{*} -->
<!--   5 &= 5 -->
<!-- \end{align} -->

<!-- \begin{equation} -->
<!--   \tag{**} -->
<!--   f\left(k\right) = \binom{n}{k} p^k\left(1-p\right)^{n-k} -->
<!--   (\#eq:binom) -->
<!-- \end{equation}  -->