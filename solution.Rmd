---
title: "Spooky Author Identification"
author: |
  | Eugene Girtcius
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    code_folding: hide
    css: styles.css
    df_print: kable
    fig_caption: yes
    fig_height: 6
    fig_width: 9
    highlight: tango
    mathjax: default
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
  # pdf_document:
  #   dev: cairo_pdf
  #   df_print: kable
  #   fig_caption: yes
  #   fig_height: 6
  #   fig_width: 9
  #   highlight: tango
  #   includes:
  #     in_header: preamble-latex.tex
  #   keep_tex: yes
  #   latex_engine: xelatex
  #   number_sections: yes
  #   toc: yes
fontsize: 12pt
geometry: left=3cm, right=3cm, top=3cm, bottom=3cm
header-includes:
- \usepackage{longtable}
- \usepackage{natbib}
- \setcitestyle{square,numbers,super}
- \usepackage{amsmath}
linkcolor: blue
documentclass: article
citecolor: blue
urlcolor: blue
---

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
      equationNumbers: {
            autoNumber: "all",
            <!-- formatNumber: function (n) {return '9.'+n} -->
      }
  }
});
</script>

```{r setup, include = F, echo = F}
knitr::opts_chunk$set(echo = T, error = F, cache.lazy = F)
# options(knitr.table.format = "latex", booktabs = T) for PDF output
```

# Task

[Spooky Author Identification](https://www.kaggle.com/c/spooky-author-identification/)

The competition dataset contains text from works of fiction written by spooky authors of the public domain: Edgar Allan Poe, HP Lovecraft and Mary Shelley. The data was prepared by chunking larger texts into sentences using CoreNLP's MaxEnt sentence tokenizer, so you may notice the odd non-sentence here and there. Your objective is to accurately identify the author of the sentences in the test set.

# Preparation {.tabset .tabset-fade .tabset-pills}

## Used libraries

```{r, message = F}
# common
library('scales')
library('gridExtra')
library('corrplot')
library("ggcorrplot")
library('tidyverse')
library('reshape2')

# NLP
library('tidytext')
library('tm')
library('SnowballC')
library('topicmodels')
library('wordcloud')
library('qdap')
library('textstem')
library('udpipe')

# models
library('xgboost')
library('caret')
library('keras')
library('fastNaiveBayes')
```

## Data loading

```{r warning = F, results = F, message = F}
train <- read_csv('data/train.csv')
test <- read_csv('data/test.csv')
sample <- read.csv('data/sample_submission.csv')
```

# Exploratory Data Analysis

## Data structure {.tabset .tabset-fade .tabset-pills}

### Train data

```{r, cache = T}
train %>%
    dplyr::slice(1:5)
```

```{r, cache = T}
glimpse(train)
```

```{r, cache = T}
summary(train)
```

```{r, cache = T}
train %>%
    dplyr::slice(which.max(nchar(text)))
```

```{r, cache = T}
train %>%
    dplyr::slice(which.min(nchar(text)))
```

```{r, cache = T}
colSums(is.na(train))
```

### Test data
 
```{r, cache = T}
test %>%
    dplyr::slice(1:5)
```

```{r, cache = T}
glimpse(test)
```

```{r, cache = T}
summary(test)
```

```{r, cache = T}
test %>%
    dplyr::slice(which.max(nchar(text)))
```

```{r, cache = T}
test %>%
    dplyr::slice(which.min(nchar(text)))
```

```{r, cache = T}
colSums(is.na(test))
```

## Class distribution

```{r, cache = T}
table(train$author) %>%
    prop.table()
```

```{r, cache = T}
train %>% 
    group_by(author) %>%
    summarise(n = n()) %>% 
    ggplot(data = ., aes(x = author, y = n, fill = author)) +
    geom_col(show.legend = F) +
    scale_fill_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
    xlab(label = 'author') +
    ylab(label = 'number of texts') +
    theme_bw()
```

```{r, cache = T}

train %>% 
    mutate(len = nchar(text)) %>% 
    ggplot(data = ., aes(x = len, fill = author)) +
    geom_histogram(binwidth = 50) +
    scale_fill_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
    facet_grid(. ~ author) +
    xlab(label = 'number of characters') +
    ylab(label = 'number of texts') +
    theme_bw()

```

```{r, cache = T}
train %>% 
    mutate(len = nchar(text)) %>% 
    group_by(author) %>% 
    summarise(`median text length` = median(len), `mean text length` = mean(len))
```

## Wordclouds {.tabset .tabset-fade .tabset-pills}

```{r, cache = T}
train %>%
    unnest_tokens(word, text) %>% 
    count(word) %>% 
    with(wordcloud(word, n, max.words = 50, scale = c(3, 1.5), random.order = F, rot.per = 0, color = 'black'))
```

```{r, cache = T}
train_clean <- train %>% 
    unnest_tokens(word, text) %>% 
    anti_join(stop_words, by = 'word')

train_clean %>%
    count(word) %>% 
    with(wordcloud(word, n, max.words = 50, scale = c(3, 1), random.order = F, rot.per = 0, color = 'black'))
```

### Lovecraft

```{r, cache = T}
train_clean %>%
    filter(author == 'HPL') %>% 
    count(word) %>% 
    with(wordcloud(word, n, max.words = 50, scale = c(3, 1), random.order = F, rot.per = 0, color = 'blue4'))
```

### Poe

```{r, cache = T}
train_clean %>%
    filter(author == 'EAP') %>% 
    count(word) %>% 
    with(wordcloud(word, n, max.words = 50, scale = c(3, 1), random.order = F, rot.per = 0, color = 'red4'))
```

### Shelley

```{r, cache = T}
train_clean %>%
    filter(author == 'MWS') %>% 
    count(word) %>% 
    with(wordcloud(word, n, max.words = 50, scale = c(3, 1), random.order = F, rot.per = 0, color = 'purple4'))
```

## Keywords {.tabset .tabset-fade .tabset-pills}

```{r, cache = T}
get_bar_plot <- function(author_id, clr, stem = F){
    
    if(stem){
        t <- train_clean %>%
            filter(author == author_id) %>% 
            mutate(word = wordStem(word))
    }else{
        t <- train_clean %>%
            filter(author == author_id)
    }
    
    t %>% 
        count(word) %>% 
        top_n(30, n) %>%
        arrange(n) %>%
        mutate(word = factor(word, levels = word)) %>%
        ggplot() +
        geom_col(aes(word, n), fill = clr) +
        ggtitle(author_id) +
        coord_flip()
}
```

### Plain keywords

```{r, cache = T}
pl1 <- get_bar_plot(author_id = 'HPL', clr = 'blue4')
pl2 <- get_bar_plot(author_id = 'EAP', clr = 'red4')
pl3 <- get_bar_plot(author_id = 'MWS', clr = 'purple4')

grid.arrange(pl1, pl2, pl3, nrow = 1)
```

### Keywords + stemming

```{r, cache = T}
pl1 <- get_bar_plot(author_id = 'HPL', clr = 'blue4', stem = T)
pl2 <- get_bar_plot(author_id = 'EAP', clr = 'red4', stem = T)
pl3 <- get_bar_plot(author_id = 'MWS', clr = 'purple4', stem = T)

grid.arrange(pl1, pl2, pl3, nrow = 1)
```

## Unique words

```{r, cache = T}
train_clean %>%
    distinct(author, word) %>% 
    count(author) %>% 
    rename(`unique word count` = n)
```

```{r, cache = T}
get_uniq_words_plot <- function(author_id){
    
    t <- train_clean %>% 
        filter(author == author_id) %>% 
        count(word) %>% 
        arrange(desc(n))
    
    t %>% 
        mutate(cumsum = cumsum(n),
               cumsum_perc = round(100 * cumsum/sum(n), digits = 2)) %>% 
        ggplot(aes(x = 1:nrow(t), y = cumsum_perc)) +
        geom_line() +
        geom_hline(yintercept = 50, color = 'black', alpha = 0.5) +
        geom_hline(yintercept = 75, color = 'yellow', alpha = 0.5) +
        geom_hline(yintercept = 90, color = 'orange', alpha = 0.5) +
        geom_hline(yintercept = 95, color = 'red', alpha = 0.5) +
        xlab('number of unique words') +
        ylab('% coverage') +
        ggtitle(author_id) +
        theme_bw()
        
}

pl1 <- get_uniq_words_plot(author_id = 'HPL')
pl2 <- get_uniq_words_plot(author_id = 'EAP')
pl3 <- get_uniq_words_plot(author_id = 'MWS')

grid.arrange(pl1, pl2, pl3, nrow = 1)
```

## Words co-occurrence {.tabset .tabset-fade .tabset-pills}

```{r, cache = T}
word_freq <- train_clean %>%
    count(author, word) %>%
    group_by(author)  %>%
    mutate(prop = n / sum(n)) %>% 
    select(-n) %>% 
    spread(author, prop)

get_word_freq_plot <- function(author_id1, author_id2){
    word_freq %>% 
        filter(!is.na(!!author_id1) & !is.na(!!author_id2)) %>% 
        mutate(clr = abs(!!author_id1 - !!author_id2)) %>%
        ggplot(aes(x = !!author_id1, y = !!author_id2, color = clr)) +
        geom_abline(color = 'gray40', lty = 2) +
        geom_jitter(alpha = 0.1, size = 2, width = 0.3, height = 0.3) +
        geom_text(aes(label = word), check_overlap = T, vjust = 1.5) +
        scale_x_log10(labels = percent_format()) +
        scale_y_log10(labels = percent_format()) +
        theme_bw() +
        theme(legend.position = 'none') +
        labs(x = author_id1, y = author_id2)
}
```

### Lovecraft & Poe

```{r, cache = T}
get_word_freq_plot(author_id1 = quo(HPL), author_id2 = quo(EAP))
```

### Lovecraft & Shelley

```{r, cache = T}
get_word_freq_plot(author_id1 = quo(HPL), author_id2 = quo(MWS))
```

### Poe & Shelley

```{r, cache = T}
get_word_freq_plot(author_id1 = quo(EAP), author_id2 = quo(MWS))
```

## Correlation

```{r, cache = T}
word_freq %>%
    select(-word) %>%
    cor(use = 'complete.obs', method = 'pearson') %>%
    corrplot(type = 'lower',
             method = 'number',
             diag = F)
```

## TF-IDF {.tabset .tabset-fade .tabset-pills}

```{r, cache = T}
tf_idf <- train_clean %>%
    count(author, word) %>% 
    bind_tf_idf(word, author, n)

get_tf_idf_plot <- function(df, cnt){
    df %>%
        arrange(desc(tf_idf)) %>%
        mutate(word = factor(word, levels = rev(unique(word)))) %>%
        top_n(cnt, tf_idf) %>%
        ggplot(aes(word, tf_idf, fill = author)) +
        scale_fill_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
        geom_col() +
        labs(x = NULL, y = 'TF-IDF values') +
        theme_bw() +
        theme(legend.position = 'top') +
        coord_flip()
}

get_tf_idf_facet_plot <- function(df, cnt){
    df %>%
        arrange(desc(tf_idf)) %>%
        mutate(word = factor(word, levels = rev(unique(word)))) %>%
        group_by(author) %>%
        top_n(cnt, tf_idf) %>%
        ungroup() %>% 
        ggplot(aes(word, tf_idf, fill = author)) +
        scale_fill_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
        geom_col() +
        labs(x = NULL, y = 'TF-IDF values') +
        theme_bw() +
        theme(legend.position = 'none') +
        facet_wrap(~author, ncol = 3, scales = 'free_y') +
        coord_flip()
}
```

### Common unigrams

```{r, cache = T}
get_tf_idf_plot(tf_idf, 20)
```

### Personal unigrams

```{r, cache = T}
get_tf_idf_facet_plot(tf_idf, 20)
```

## Bigrams {.tabset .tabset-fade .tabset-pills}

```{r, cache = T}
train_clean_bigram <- train %>% 
    unnest_tokens(word, text, token = 'ngrams', n = 2) %>% 
    separate(word, c('word1', 'word2'), sep = ' ') %>% 
    anti_join(stop_words, by = c('word1' = 'word')) %>% 
    anti_join(stop_words, by = c('word2' = 'word')) %>% 
    unite(word, c('word1', 'word2'), sep = ' ')

tf_idf_bigram <- train_clean_bigram %>%
    count(author, word) %>% 
    bind_tf_idf(word, author, n)
```

### Common bigrams

```{r, cache = T}
get_tf_idf_plot(tf_idf_bigram, 20)
```

### Personal bigrams

```{r, cache = T}
get_tf_idf_facet_plot(tf_idf_bigram, 15)
```

## Trigrams {.tabset .tabset-fade .tabset-pills}

```{r, cache = T}
train_clean_trigram <- train %>% 
    unnest_tokens(word, text, token = 'ngrams', n = 3) %>% 
    separate(word, c('word1', 'word2', 'word3'), sep = ' ') %>% 
    anti_join(stop_words, by = c('word1' = 'word')) %>% 
    anti_join(stop_words, by = c('word2' = 'word')) %>% 
    anti_join(stop_words, by = c('word3' = 'word')) %>% 
    unite(word, c('word1', 'word2', 'word3'), sep = ' ')

tf_idf_trigram <- train_clean_trigram %>%
    count(author, word) %>% 
    bind_tf_idf(word, author, n)
```

### Common trigrams

```{r, cache = T}
get_tf_idf_plot(tf_idf_trigram, 10)
```

### Personal trigrams

```{r, cache = T}
get_tf_idf_facet_plot(tf_idf_trigram, 6)
```

## LDA

### 3 topics

```{r, cache = T}

train_dtm <-  train %>% 
    mutate(text = as.character(text), id = as.character(id), id = paste(author, id, sep="_")) %>% 
    unnest_tokens(word, text) %>% 
    anti_join(stop_words, by = 'word') %>% 
    count(id, word) %>% 
    cast_dtm(id, word, n)

train_lda <- LDA(train_dtm, k = 3, control = list(seed = 42))

train_topics <- tidy(train_lda, matrix = 'beta')

train_topics %>%
    group_by(topic) %>%
    top_n(20, beta) %>%
    ungroup() %>%
    arrange(topic, -beta) %>%
    mutate(term = reorder(term, beta)) %>%
    ggplot(aes(term, beta, fill = factor(topic))) +
    geom_col(show.legend = F) +
    facet_wrap(~ topic, scales = 'free', ncol = 5) +
    coord_flip() +
    theme_bw()
```

```{r, cache = T}
gamma_extractor <- function(lda_model) {
  tidy(lda_model, matrix="gamma") %>%
    spread(topic, gamma) %>%
    separate(document, c("author", "id"),
             sep="_",
             convert=TRUE)         %>%
    mutate(top = NA)               %>%
    mutate(prediction = NA)
}

gamma_lda <- gamma_extractor(train_lda)
length_lda <- nrow(gamma_lda)

for (i in 1:length_lda) {
  gamma_lda$top[i]        <- max(gamma_lda[i, 3:5])
  gamma_lda$prediction[i] <- match(gamma_lda[i,6], gamma_lda[i,3:5])
}

table(gamma_lda$author, gamma_lda$prediction)
```

### 9 topics

```{r, cache = T}
train_lda <- LDA(train_dtm, k = 9, control = list(seed = 42))

train_topics <- tidy(train_lda, matrix = 'beta')

train_topics %>%
    group_by(topic) %>%
    top_n(7, beta) %>%
    ungroup() %>%
    arrange(topic, -beta) %>%
    mutate(term = reorder(term, beta)) %>%
    ggplot(aes(term, beta, fill = factor(topic))) +
    geom_col(show.legend = F) +
    facet_wrap(~ topic, scales = 'free', ncol = 3) +
    coord_flip() +
    theme_bw()
```

```{r, cache = T}
gamma_lda <- gamma_extractor(train_lda)
length_lda <- nrow(gamma_lda)

for (i in 1:length_lda) {
  gamma_lda$top[i]        <- max(gamma_lda[i, 3:11])
  gamma_lda$prediction[i] <- match(gamma_lda[i,12], gamma_lda[i,3:11])
}

table(gamma_lda$author, gamma_lda$prediction)
```

# Feature engineering

## Stylometric features

```{r, cache = T}

train_stylometry <- train %>% 
    mutate(ascii_text = iconv(text, to='ASCII//TRANSLIT'),
           ascii_text = ifelse(is.na(ascii_text), text, ascii_text),
           ascii_text = str_replace_all(ascii_text, c("ΥΠΝΟΣ" = "Hypnos", "Οἶδα Οἶδα" = "Oida Oida")),
           word_count = str_count(text, '\\w+'),
           nchar = nchar(text),
           nchar_per_word = nchar/word_count,
           syll_count = syllable_sum(ascii_text),
           nsyll_per_word = syll_count/word_count,
           tc_count = str_count(text, '[A-Z][a-z]+'),
           uc_count = str_count(text, '[A-Z][A-Z]+'),
           punctuation_count = str_count(text, '[:punct:]')) %>% 
    select(-c(text, ascii_text, author))

test_stylometry <- test %>% 
    mutate(ascii_text = iconv(text, to='ASCII//TRANSLIT'),
           word_count = str_count(text, '\\w+'),
           nchar = nchar(text),
           nchar_per_word = nchar/word_count,
           syll_count = syllable_sum(ascii_text),
           nsyll_per_word = syll_count/word_count,
           tc_count = str_count(text, '[A-Z][a-z]+'),
           uc_count = str_count(text, '[A-Z][A-Z]+'),
           punctuation_count = str_count(text, '[:punct:]')) %>% 
    select(-c(text, ascii_text))
 
```

```{r, cache = T}
 
train_tmp <- train %>%
    unnest_tokens(term, text, token = 'ngrams', n=1) %>%
    mutate(stop_word_ind = as.integer(term %in% stop_words$word),
           ascii_term = str_replace_all(term, c("υπνος" = "hypnos", "οἶδα" = "oida")),
           ascii_term = iconv(ascii_term, to='ASCII//TRANSLIT'),
           ascii_term = ifelse(is.na(ascii_term), term, ascii_term),
           syll_count = syllable_sum(ascii_term, parallel = T),
           word_length = nchar(term)) %>%
    group_by(id) %>%
    summarise(nStopWord = sum(stop_word_ind),
              nUniqueWord = n_distinct(term),
              avg_word_length = mean(word_length),
              ngt6lWord = sum(word_length > 6),
              n1SylWord = sum(syll_count == 1),
              nPolySylWord = sum(syll_count > 2)
              )

test_tmp <- test %>%
    unnest_tokens(term, text, token = 'ngrams', n=1) %>%
    mutate(stop_word_ind = as.integer(term %in% stop_words$word),
           ascii_term = str_replace_all(term, c("υπνος" = "hypnos", "οἶδα" = "oida")),
           ascii_term = iconv(ascii_term, to='ASCII//TRANSLIT'),
           ascii_term = ifelse(is.na(ascii_term), term, ascii_term),
           syll_count = syllable_sum(ascii_term, parallel = T),
           word_length = nchar(term)) %>%
    group_by(id) %>%
    summarise(nStopWord = sum(stop_word_ind),
              nUniqueWord = n_distinct(term),
              avg_word_length = mean(word_length),
              ngt6lWord = sum(word_length > 6),
              n1SylWord = sum(syll_count == 1),
              nPolySylWord = sum(syll_count > 2)
              )
```

```{r, cache = T}
train_stylometry <- train_stylometry %>% 
    left_join(train_tmp, by = 'id') %>%
    mutate(unique_r = nUniqueWord/word_count,
           w_p = word_count - punctuation_count,
           w_p_r = w_p/word_count,
           stop_r = nStopWord/word_count,
           w_p_stop = w_p - nStopWord,
           w_p_stop_r = w_p_stop/word_count,
           num_words_upper_r = uc_count/word_count,
           num_words_title_r = tc_count/word_count)

test_stylometry <- test_stylometry %>% 
    left_join(test_tmp, by = 'id') %>%
    mutate(unique_r = nUniqueWord/word_count,
           w_p = word_count - punctuation_count,
           w_p_r = w_p/word_count,
           stop_r = nStopWord/word_count,
           w_p_stop = w_p - nStopWord,
           w_p_stop_r = w_p_stop/word_count,
           num_words_upper_r = uc_count/word_count,
           num_words_title_r = tc_count/word_count)

```

```{r, cache = T}
train  %>%
    left_join(train_stylometry, by = 'id') %>%
    select(-c(text, id)) %>%
    filter(nchar < 1000) %>%
    group_by(author) %>%
    summarise_all(mean) %>%
    gather(feature, value, -author) %>%
    ggplot(aes(x = feature, y = value, color = author)) +
    scale_color_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
    geom_point(alpha = 0.6, size = 3) +
    scale_y_log10() +
    labs(x = 'feature', y = 'mean value by author') +
    coord_flip() +
    theme_bw()
```

## Tonality-based features {.tabset .tabset-fade .tabset-pills}

### Verb tonality and intensifiers

```{r, cache = T}
thought_verbs <- c('analyze', 'apprehend', 'assume', 'believe', 'calculate', 'cerebrate', 'cogitate',
                   'comprehend', 'conceive', 'concentrate', 'conceptualize', 'conclude', 'consider',
                   'construe', 'contemplate', 'deduce', 'deem', 'delibrate', 'desire', 'diagnose',
                   'doubt', 'envisage', 'envision', 'evaluate', 'excogitate', 'extrapolate', 'fantasize',
                   'forget', 'forgive', 'formulate', 'hate', 'hypothesize', 'imagine', 'infer', 
                   'intellectualize', 'intrigue', 'guess', 'introspect', 'judge', 'know', 'love', 
                   'lucubrate', 'marvel', 'meditate', 'note', 'notice', 'opine', 'perpend', 'philosophize',
                   'ponder', 'question', 'ratiocinate', 'rationalize', 'realize', 'reason', 'recollect', 
                   'reflect', 'remember', 'reminisce', 'retrospect', 'ruminate', 'sense', 'speculate',
                   'stew', 'strategize', 'suppose', 'suspect', 'syllogize', 'theorize', 'think', 
                   'understand', 'visualize', 'want', 'weigh', 'wonder')

loud_verbs <- c('cry', 'exclaim', 'shout', 'roar', 'scream', 'shriek', 'vociferated', 'bawl',
                'call', 'ejaculate', 'retort', 'proclaim', 'announce', 'protest', 'accost', 'declare')

neutral_verbs <- c('say', 'reply', 'observe', 'rejoin', 'ask', 'answer', 'return', 'repeat', 'remark',
                   'enquire', 'respond', 'suggest', 'explain', 'utter', 'mention')

quiet_verbs <- c('whisper', 'murmur', 'sigh', 'grumble', 'mumble', 'mutter', 'whimper', 'hush', 'falter',
                 'stammer', 'tremble', 'gasp', 'shudder')

qualifiers <- c('very', 'too', 'so', 'quite', 'rather', 'little', 'pretty', 'somewhat', 'various', 'almost', 
                'much', 'just', 'indeed', 'still', 'even', 'a lot', 'kind of', 'sort of')

train_tmp <- train %>%
    unnest_tokens(term, text, token = 'ngrams', n=1) %>% 
    bind_rows(train %>% unnest_tokens(term, text, token = 'ngrams', n=2)) %>%
    mutate(term = lemmatize_words(term),
           qualifier_ind = as.integer(term %in% qualifiers),
           thought_verbs_ind = as.integer(term %in% thought_verbs),
           loud_verbs_ind = as.integer(term %in% loud_verbs),
           neutral_verbs_ind = as.integer(term %in% neutral_verbs),
           quiet_verbs_ind = as.integer(term %in% quiet_verbs)) %>%
    group_by(id) %>%
    summarise(qualifier_count = sum(qualifier_ind),
              thought_verbs_count = sum(thought_verbs_ind),
              loud_verbs_count = sum(loud_verbs_ind),
              neutral_verbs_count = sum(neutral_verbs_ind),
              quiet_verbs_count = sum(quiet_verbs_ind))

test_tmp <- test %>%
    unnest_tokens(term, text, token = 'ngrams', n=1) %>% 
    bind_rows(test %>% unnest_tokens(term, text, token = 'ngrams', n=2)) %>%
    mutate(term = lemmatize_words(term),
           qualifier_ind = as.integer(term %in% qualifiers),
           thought_verbs_ind = as.integer(term %in% thought_verbs),
           loud_verbs_ind = as.integer(term %in% loud_verbs),
           neutral_verbs_ind = as.integer(term %in% neutral_verbs),
           quiet_verbs_ind = as.integer(term %in% quiet_verbs)) %>%
    group_by(id) %>%
    summarise(qualifier_count = sum(qualifier_ind),
              thought_verbs_count = sum(thought_verbs_ind),
              loud_verbs_count = sum(loud_verbs_ind),
              neutral_verbs_count = sum(neutral_verbs_ind),
              quiet_verbs_count = sum(quiet_verbs_ind))
 
train_stylometry <- train_stylometry %>% 
    left_join(train_tmp, by = 'id')

test_stylometry <- test_stylometry %>% 
    left_join(test_tmp, by = 'id')
```

```{r, cache = T}
train  %>%
    left_join(train_tmp, by = 'id') %>%
    select(-c(text, id)) %>%
    group_by(author) %>%
    summarise_all(mean) %>%
    gather(feature, value, -author) %>%
    ggplot(aes(x = feature, y = value, color = author)) +
    scale_color_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
    geom_point(alpha = 0.6, size = 3) +
    scale_y_log10() +
    labs(x = 'feature', y = 'mean value by author') +
    coord_flip() +
    theme_bw()
```

### Sentiment analysis

```{r, cache = T}
train_senti <- train %>%
    unnest_tokens(word, text) %>%
    inner_join(get_sentiments('nrc'), by = 'word') %>%
    count(id, sentiment) %>%
    spread(sentiment, n, sep = '_', fill = 0)

test_senti <- test %>%
    unnest_tokens(word, text) %>%
    inner_join(get_sentiments('nrc'), by = 'word') %>%
    count(id, sentiment) %>%
    spread(sentiment, n, sep = '_', fill = 0)

afinn_sentiment <- train %>%
    select(-author) %>%
    bind_rows(test) %>%
    unnest_tokens(word, text) %>%
    inner_join(get_sentiments('afinn'), by = 'word') %>%
    group_by(id) %>%
    summarise(sentiment_afinn = mean(value))

train_senti <- train_senti %>%
    left_join(afinn_sentiment, by = 'id') %>%
    replace_na(list(sentiment_afinn = 0))

test_senti <- test_senti %>%
    left_join(afinn_sentiment, by = 'id') %>%
    replace_na(list(sentiment_afinn = 0))

```

```{r, cache = T}
train %>%
    right_join(train_senti, by = 'id') %>%
    select(-c(text, id)) %>%
    group_by(author) %>%
    summarise_all(mean) %>%
    gather(feature, value, -author) %>%
    ggplot(aes(x = feature, y = value, color = author)) +
    scale_color_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
    geom_point(alpha = 0.6, size = 3) +
    labs(x = 'feature', y = 'mean value by author') +
    coord_flip() +
    theme_bw()
```

## POS-tagging {.tabset .tabset-fade .tabset-pills}

```{r, cache = T}
train <- train %>% 
    mutate(clean_text = str_replace(text, '^, ', ''),
           clean_text = str_replace(text, '^\\.\" ', ''),
           clean_text = str_replace_all(text, '\"', ''))

test <- test %>% 
    mutate(clean_text = str_replace_all(text, '\"', ''))
```

```{r, cache = T, message = F}
udmodel <- udpipe_download_model(language = 'english')
udmodel <- udpipe_load_model(file = udmodel$file_model)
```

```{r, cache = T}
x <- udpipe_annotate(object = udmodel, x = train$clean_text,  doc_id = train$id)
x <- as.data.frame(x)
```

```{r, cache = T}
y <- udpipe_annotate(object = udmodel, x = test$clean_text,  doc_id = test$id)
y <- as.data.frame(y)
```

```{r, cache = T}
train_udp_pos_count <- x %>%
    count(doc_id, xpos) %>%
    spread(xpos, n, fill = 0) %>%
    rename(id = doc_id)

test_udp_pos_count <- y %>%
    count(doc_id, xpos) %>%
    spread(xpos, n, fill = 0) %>% 
    select(-c(`''`, `,`, `.`, `:`, '``')) %>%
    rename(id = doc_id)

train_udp_pos_count <- train_udp_pos_count %>% 
    select(intersect(colnames(.), colnames(test_udp_pos_count)))

train_udp_upos_count <- x %>%
    count(doc_id, upos) %>%
    spread(upos, n, fill = 0) %>% 
    select(-PUNCT) %>%
    rename(id = doc_id)

test_udp_upos_count <- y %>%
    count(doc_id, upos) %>%
    spread(upos, n, fill = 0) %>% 
    select(-PUNCT) %>%
    rename(id = doc_id)

train_pos_tagged <- x
test_pos_tagged <- y
```

### Language-specific POS tags

```{r, cache = T}
train %>%  
    left_join(train_udp_pos_count, by = 'id') %>%
    select(-c(text, id, clean_text)) %>%
    group_by(author) %>%
    summarise_all(mean) %>%
    gather(feature, value, -author) %>%
    ggplot(aes(x = feature, y = value, color = author)) +
    scale_color_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
    geom_point(alpha = 0.6, size = 3) +
    labs(x = 'feature', y = 'mean value by author') +
    coord_flip() +
    theme_bw()
```

### Universal POS tags

```{r, cache = T}
train %>%  
    left_join(train_udp_upos_count, by = 'id') %>%
    select(-c(text, id, clean_text)) %>%
    group_by(author) %>%
    summarise_all(mean) %>%
    gather(feature, value, -author) %>%
    ggplot(aes(x = feature, y = value, color = author)) +
    scale_color_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
    geom_point(alpha = 0.6, size = 3) +
    labs(x = 'feature', y = 'mean value by author') +
    coord_flip() +
    theme_bw()
```

## N-gram-based features

```{r, cache = T}
author_unigrams_tfidf <- train %>%
    unnest_tokens(term, text, token = 'ngrams', n=1) %>%
    count(author, term) %>%
    bind_tf_idf(term, author, n)

author_bigrams_tfidf <- train %>%
    unnest_tokens(term, text, token = 'ngrams', n=2) %>%
    count(author, term) %>%
    bind_tf_idf(term, author, n)

author_trigrams_tfidf <- train %>%
    unnest_tokens(term, text, token = 'ngrams', n=3) %>%
    count(author, term) %>%
    bind_tf_idf(term, author, n)

author_tetragrams_tfidf <- train %>%
    unnest_tokens(term, text, token = 'ngrams', n=4) %>%
    count(author, term) %>%
    bind_tf_idf(term, author, n)

author_char_bigrams_tfidf <- train %>% 
    unnest_tokens(shingle, text, token = 'character_shingles', n=2, strip_non_alphanum = F) %>%
    count(author, shingle) %>%
    bind_tf_idf(shingle, author, n)

author_char_trigrams_tfidf <- train %>% 
    unnest_tokens(shingle, text, token = 'character_shingles', n=3, strip_non_alphanum = F) %>%
    count(author, shingle) %>%
    bind_tf_idf(shingle, author, n)

author_char_tetragrams_tfidf <- train %>% 
    unnest_tokens(shingle, text, token = 'character_shingles', n=4, strip_non_alphanum = F) %>%
    count(author, shingle) %>%
    bind_tf_idf(shingle, author, n)

author_char_pentagrams_tfidf <- train %>% 
    unnest_tokens(shingle, text, token = 'character_shingles', n=5, strip_non_alphanum = F) %>%
    count(author, shingle) %>%
    bind_tf_idf(shingle, author, n)
```

```{r, cache = T}
get_author_ngrams <- function(df, author_id){
    df %>% filter(idf == max(idf, na.rm = T) & author == author_id & n > 25) %>% .[,2]
}

df_ngram <- list(author_unigrams_tfidf, author_bigrams_tfidf, author_trigrams_tfidf, author_tetragrams_tfidf,
                author_char_bigrams_tfidf, author_char_trigrams_tfidf, author_char_tetragrams_tfidf,
                author_char_pentagrams_tfidf)

eap <- mapply(get_author_ngrams, df_ngram, rep('EAP', 8)) %>% unlist() %>% unique()
hpl <- mapply(get_author_ngrams, df_ngram, rep('HPL', 8)) %>% unlist() %>% unique()
mws <- mapply(get_author_ngrams, df_ngram, rep('MWS', 8)) %>% unlist() %>% unique()

get_total_ngrams <- function(df){
    map_df(1:4, ~ unnest_tokens(df, term, text, token = 'ngrams', n = .x)) %>%
    bind_rows(map_df(2:5, ~ unnest_tokens(df, term, text, token = 'character_shingles', n = .x,
                                          strip_non_alphanum = F))) %>%
    mutate(EAP_only_ind = as.integer(term %in% eap),
           HPL_only_ind = as.integer(term %in% hpl),
           MWS_only_ind = as.integer(term %in% mws)) %>%
    group_by(id) %>%
    summarise(EAP_only_count = sum(EAP_only_ind),
              HPL_only_count = sum(HPL_only_ind),
              MWS_only_count = sum(MWS_only_ind))
}

train_author_only <- get_total_ngrams(train)
test_author_only <- get_total_ngrams(test)

get_author_pair_ngrams <- function(df, author_id1, author_id2){
    df %>% filter(author == author_id1 | author == author_id2, idf == log(1.5)) %>%
        group_by_at(2) %>% 
        count(wt = n) %>% filter(n > 50) %>% .[,1]
}

eap_hpl <- mapply(get_author_pair_ngrams, df_ngram, rep('EAP', 8), rep('HPL', 8)) %>% unlist() %>% unique()
eap_mws <- mapply(get_author_pair_ngrams, df_ngram, rep('EAP', 8), rep('MWS', 8)) %>% unlist() %>% unique()
hpl_mws <- mapply(get_author_pair_ngrams, df_ngram, rep('HPL', 8), rep('MWS', 8)) %>% unlist() %>% unique()

get_total_pair_ngrams <- function(df){
    map_df(1:4, ~ unnest_tokens(df, term, text, token = 'ngrams', n = .x)) %>%
    bind_rows(map_df(2:5, ~ unnest_tokens(df, term, text, token = 'character_shingles', n = .x,
                                          strip_non_alphanum = F))) %>%
    mutate(EAP_HPL_only_ind = as.integer(term %in% eap_hpl),
           EAP_MWS_only_ind = as.integer(term %in% eap_mws),
           HPL_MWS_only_ind = as.integer(term %in% hpl_mws)) %>%
    group_by(id) %>%
    summarise(EAP_HPL_only_count = sum(EAP_HPL_only_ind),
              EAP_MWS_only_count = sum(EAP_MWS_only_ind),
              HPL_MWS_only_count = sum(HPL_MWS_only_ind))
}

train_author_pair_only <- get_total_pair_ngrams(train)
test_author_pair_only <- get_total_pair_ngrams(test)
```

```{r, cache = T}
train %>% 
    left_join(train_author_only, by = 'id') %>%
    left_join(train_author_pair_only, by = 'id') %>%
    select(-c(text, id, clean_text)) %>%
    group_by(author) %>%
    summarise_all(mean) %>%
    gather(feature, value, -author) %>%
    ggplot(aes(x = feature, y = value, color = author)) +
    scale_color_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
    geom_point(alpha = 0.6, size = 3) +
    labs(x = 'feature', y = 'mean value by author') +
    coord_flip() +
    theme_bw()


```

## Gender features

```{r, cache = T}
get_gender_plot <- function(m, f){
    train %>%
        unnest_tokens(word, text) %>%
        filter((word == m) | (word == f)) %>%
        mutate(word = as.factor(word)) %>%
        mutate(word = fct_relevel(word, f, m)) %>%
        ggplot(aes(word, fill = author)) +
        geom_bar(position = 'dodge') +
        scale_fill_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
        theme_bw()
}

pl1 <- get_gender_plot('man', 'woman')
pl2 <- get_gender_plot('he', 'she')
pl3 <- get_gender_plot('him', 'her')

pl4 <- train %>%
    unnest_tokens(word, text) %>%
    mutate(male = (word %in% c('he', 'him', 'his', 'male', 'man', 'gentleman', 'sir', 'lord', 'men'))) %>%
    mutate(female = (word %in% c('she', 'her', 'hers', 'female', 'woman', 'lady', 'madam', 'women' ))) %>%
    unite(sex, male, female) %>%
    mutate(sex = fct_recode(as.factor(sex), male = 'TRUE_FALSE', 
                          female = 'FALSE_TRUE', other = 'FALSE_FALSE')) %>%
    filter(sex != 'other') %>%
    ggplot(aes(sex, fill = author)) +
    labs(x = 'gender indicators') +
    geom_bar(position = 'dodge') +
    scale_fill_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
    theme_bw()

grid.arrange(pl1, pl2, pl3, pl4, nrow = 2)

train_gender <- train %>%
    unnest_tokens(word, text) %>% 
    mutate(male = (word %in% c('he', 'him', 'his', 'male', 'man', 'gentleman', 'sir', 'lord', 'men'))) %>%
    mutate(female = (word %in% c('she', 'her', 'hers', 'female', 'woman', 'lady', 'madam', 'women' ))) %>% 
    group_by(id) %>%
    summarise(male = sum(male), female = sum(female))

test_gender <- test %>%
    unnest_tokens(word, text) %>% 
    mutate(male = (word %in% c('he', 'him', 'his', 'male', 'man', 'gentleman', 'sir', 'lord', 'men'))) %>%
    mutate(female = (word %in% c('she', 'her', 'hers', 'female', 'woman', 'lady', 'madam', 'women' ))) %>% 
    group_by(id) %>%
    summarise(male = sum(male), female = sum(female))
```

## Alliterations and assonance

```{r, cache = T}
get_alliterations <- function(df, stop = F){
    if(stop){
        df <- df %>%
            select(id, text) %>% 
            unnest_tokens(word, text) %>% 
            anti_join(stop_words, by = 'word')
    }else{
        df <- df %>%
            select(id, text) %>% 
            unnest_tokens(word, text)
    }

    df %>% 
        mutate(first = str_sub(word, start = 1, end = 1),
             f_lead_1 = lead(first, n = 1),
             f_lead_2 = lead(first, n = 2),
             id_lead_1 = lead(id, n = 1),
             id_lead_2 = lead(id, n = 2),
             allit2 = first == f_lead_1 & id == id_lead_1,
             allit3 = first == f_lead_1 & first == f_lead_2 & id == id_lead_1 & id == id_lead_2
             ) %>%
        filter(!is.na(allit2)) %>%
        group_by(id, allit2) %>%
        count() %>%
        spread(allit2, n) %>%
        mutate(has_allit = ifelse(!is.na(`TRUE`), 1, 0)) %>%
        select(id, has_allit)
}

train_alliterations <- get_alliterations(train, stop = T)
test_alliterations <- get_alliterations(test, stop = T)

get_allit_plot <- function(df, title){
    df %>% 
        left_join(train, by = 'id') %>% 
        group_by(author, has_allit) %>% 
        count() %>% 
        ungroup() %>% 
        (function(df) left_join(df, df %>% group_by(author) %>% summarise(s = sum(n)), by = 'author')) %>% 
        mutate(n = (n/s) * 100) %>% 
        filter(has_allit == T) %>% 
        ggplot(aes(x = author, y = n, fill = author)) +
        geom_col() +
        scale_fill_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
        labs(y = "fraction of sentences with alliterations") +
        ggtitle(title) +
        theme_bw()
}

pl1 <- get_allit_plot(get_alliterations(train, stop = T), title = 'without stop words')
pl2 <- get_allit_plot(get_alliterations(train, stop = F), title = 'with stop words')

grid.arrange(pl1, pl2, nrow = 1)
```

# Modeling

## Quality measurement

```{r, cache = T}
MultiLogLoss <- function(y_pred, y_true) {
    if (is.matrix(y_true) == FALSE) {
        y_true <- model.matrix(~ 0 + ., data.frame(as.character(y_true)))
    }
    y_pred <- as.matrix(y_pred)
    eps <- 1e-15
    N <- nrow(y_pred)
    y_pred <- pmax(pmin(y_pred, 1 - eps), eps)
    MultiLogLoss <- (-1 / N) * sum(y_true * log(y_pred))
    return(MultiLogLoss)
}
```

## All features

```{r, cache = T}
train_total <- train %>% 
    select(-c(text, clean_text)) %>% 
    left_join(train_stylometry, by = 'id') %>% 
    left_join(train_senti, by = 'id') %>% 
    left_join(train_udp_pos_count, by = 'id') %>% 
    left_join(train_udp_upos_count, by = 'id') %>% 
    left_join(train_gender, by = 'id') %>% 
    left_join(train_author_only, by = 'id') %>% 
    left_join(train_author_pair_only, by = 'id') %>% 
    left_join(train_alliterations, by = 'id') 
train_total[is.na(train_total)] <- 0

test_total <- test %>% 
    select(-c(text, clean_text)) %>% 
    left_join(test_stylometry, by = 'id') %>% 
    left_join(test_senti, by = 'id') %>% 
    left_join(test_udp_pos_count, by = 'id') %>% 
    left_join(test_udp_upos_count, by = 'id') %>% 
    left_join(test_gender, by = 'id') %>% 
    left_join(test_author_only, by = 'id') %>% 
    left_join(test_author_pair_only, by = 'id') %>% 
    left_join(test_alliterations, by = 'id')
test_total[is.na(test_total)] <- 0
```

## Naive Bayes classifier

```{r, cache = T}
set.seed(42)
folds <- createFolds(train$author, k = 5)
y_train <- to_categorical(as.integer(as.factor(train$author)), num_classes = NULL)
y_train <- y_train[,2:4]
```

```{r, cache = T, message = F, warning = F}
df <- train %>%
    select(-c(author)) %>% 
    unnest_tokens(token, text, token = 'words', to_lower = F) %>% 
    select(-id) %>%
    count(token) %>%
    .$token
    
df_cnt <- train %>%
    select(-c(author)) %>%
    bind_rows(test) %>%
    unnest_tokens(token, text, token = 'words', to_lower = F) %>%
    count(id, token) %>%
    filter(token %in% df) %>%
    cast_dtm(id, token, n)
    # cast_dtm(id, token, n, weighting = tm::weightTfIdf)

x_train_df <-  df_cnt[train$id,] %>% as.matrix
x_test_df <- df_cnt[test$id,] %>% as.matrix
rm(df, df_cnt)
    
model_fnb <- fnb.train(x_train_df, train$author, laplace = 1)
# MultiLogLoss(y_pred = predict(model_fnb, x_train_df, type = 'raw'), y_true = y_train)

fnb <- predict(model_fnb, x_test_df, type = 'raw')
read_csv(file = 'data/sample_submission.csv') %>% 
    select(id) %>% 
    bind_cols(., fnb %>% as_tibble()) %>% 
    write_csv('submissions/sub_fnb.csv')
```

## Neural Networks

```{r, cache = T}
nn_model <- keras_model_sequential() %>%
    layer_dense(units = 16, activation = 'relu', input_shape = 19579) %>% 
    layer_dense(units = 16, activation = 'relu') %>%
    layer_dense(units = 3, activation = 'softmax')

nn_model %>% compile(
    loss = 'categorical_crossentropy',
    optimizer = 'rmsprop',
    metrics = c('accuracy')
)
nn_model
```

### N-grams {.tabset .tabset-fade .tabset-pills}

```{r, cache = T}
get_ngram_df <- function(in_token, in_n, in_cnt){
    df <- train %>%
        select(-author) %>% 
        unnest_tokens(token, text, token = in_token, n = in_n, to_lower = F) %>% 
        select(-id) %>%
        count(token) %>%
        filter(n > in_cnt) %>%
        .$token
    
    df_cnt <- train %>%
        select(-author) %>%
        bind_rows(test) %>%
        unnest_tokens(token, text, token = in_token, n = in_n, to_lower = F) %>%
        count(id, token) %>%
        filter(token %in% df) %>%
        cast_dtm(id, token, n)
    
    if(in_cnt != 0){
        rowsRemoved <- setdiff(c(train$id,test$id),rownames(df_cnt))
        allZeros <- matrix(0, length(rowsRemoved), ncol(df_cnt), 
                           dimnames = list(rowsRemoved, colnames(df_cnt)))
        df_cnt <- df_cnt %>% rbind(allZeros)
        rm(rowsRemoved,allZeros)
    }
    
    x_train_df <- df_cnt[train$id,] %>% as.matrix
    x_test_df <- df_cnt[test$id,] %>% as.matrix
    rm(df, df_cnt)
        
    return(list(x_train_df, x_test_df))
}

get_ngram_model <- function(df_train, df_test, fold, y_train, filepath){
    nn_model <- keras_model_sequential() %>%
        layer_dense(units = 16, activation = 'relu', input_shape = ncol(df_train)) %>% 
        layer_dense(units = 16, activation = 'relu') %>%
        layer_dense(units = 3, activation = 'softmax')
    
    nn_model %>% compile(
        loss = 'categorical_crossentropy',
        optimizer = 'rmsprop',
        metrics = c('accuracy')
    )
    
    frozen_nn_model <- nn_model %>%
        fit(
            df_train[-fold,], y_train[-fold,],
            batch_size = 2^9,
            epochs = 20,
            validation_split = 0.1,
            verbose = F,
            callbacks = list(
                callback_early_stopping(monitor = 'val_loss', patience = 2),
                callback_model_checkpoint(
                    filepath = paste0(filepath, '.hdf5'),
                    monitor = 'val_loss',
                    mode = 'min',
                    save_best_only = T)
          )
    )
    nn_model <- load_model_hdf5(paste0(filepath, '.hdf5'))
    
    train_pred <- nn_model %>%
        predict(df_train[fold,])
    test_pred <- nn_model %>%
        predict(df_test)
    fold_eval <- nn_model %>%
        evaluate(df_train[fold,], y_train[fold,])
    out <- list(train_pred = train_pred, test_pred = test_pred, logloss = fold_eval[["loss"]], acc = fold_eval[["accuracy"]] )
    k_clear_session()
    return(out)
}

get_ngram_predictions <- function(ngram_df, filepath){
    train_count <- matrix(0, nrow = nrow(train), ncol = 3)
    test_count <- matrix(0, nrow = nrow(test), ncol = 3)
    metrics_count <- matrix(0, 5, 2)

    for(i in 1:5){
        results_count <- get_ngram_model(ngram_df[[1]], ngram_df[[2]], folds[[i]], y_train, filepath)
        train_count[folds[[i]], ] <- results_count$train_pred
        test_count <- test_count + (results_count$test_pred)/5
        metrics_count[i,1] <- results_count$logloss
        metrics_count[i,2] <- results_count$acc
        gc()
    }   
  
    train_count <- train_count %>%
        as.data.frame()
    test_count <- test_count %>%
        as.data.frame()
    metrics_count <- metrics_count %>%
        as.data.frame()
    rownames(metrics_count) <- paste0("fold ", 1:5, ":")
    return(list(train_count, test_count, metrics_count))
}
```

#### Unigrams

```{r, message = F, warning = F, cache = T}
ngram_df <- get_ngram_df(in_token = 'ngrams', in_n = 1, in_cnt = 0)
ngram_predictions <- get_ngram_predictions(ngram_df, 'word_1g_count')
train_word_1g_count <- ngram_predictions[[1]] %>%  
    rename(word_1g_count_EAP=V1, word_1g_count_HPL=V2, word_1g_count_MWS=V3)
test_word_1g_count <- ngram_predictions[[2]] %>% 
    rename(word_1g_count_EAP=V1, word_1g_count_HPL=V2, word_1g_count_MWS=V3)
metrics_word_1g_count <- ngram_predictions[[3]] %>% 
    rename(logloss=V1, acc=V2)
rm(ngram_df, ngram_predictions)
invisible(gc())
metrics_word_1g_count
```

#### Bigrams

```{r, message = F, warning = F, cache = T}
ngram_df <- get_ngram_df(in_token = 'ngrams', in_n = 2, in_cnt = 2)
ngram_predictions <- get_ngram_predictions(ngram_df, 'word_2g_count')
train_word_2g_count <- ngram_predictions[[1]] %>%  
    rename(word_2g_count_EAP=V1, word_2g_count_HPL=V2, word_2g_count_MWS=V3)
test_word_2g_count <- ngram_predictions[[2]] %>% 
    rename(word_2g_count_EAP=V1, word_2g_count_HPL=V2, word_2g_count_MWS=V3)
metrics_word_2g_count <- ngram_predictions[[3]] %>% 
    rename(logloss=V1, acc=V2)
rm(ngram_df, ngram_predictions)
invisible(gc())
metrics_word_2g_count
```

#### Trigrams

```{r, message = F, warning = F, cache = T}
ngram_df <- get_ngram_df(in_token = 'ngrams', in_n = 3, in_cnt = 2)
ngram_predictions <- get_ngram_predictions(ngram_df, 'word_3g_count')
train_word_3g_count <- ngram_predictions[[1]] %>%  
    rename(word_3g_count_EAP=V1, word_3g_count_HPL=V2, word_3g_count_MWS=V3)
test_word_3g_count <- ngram_predictions[[2]] %>% 
    rename(word_3g_count_EAP=V1, word_3g_count_HPL=V2, word_3g_count_MWS=V3)
metrics_word_3g_count <- ngram_predictions[[3]] %>% 
    rename(logloss=V1, acc=V2)
rm(ngram_df, ngram_predictions)
invisible(gc())
metrics_word_3g_count
```

#### 4-grams

```{r, message = F, warning = F, cache = T}
ngram_df <- get_ngram_df(in_token = 'ngrams', in_n = 4, in_cnt = 2)
ngram_predictions <- get_ngram_predictions(ngram_df, 'word_4g_count')
train_word_4g_count <- ngram_predictions[[1]] %>%  
    rename(word_4g_count_EAP=V1, word_4g_count_HPL=V2, word_4g_count_MWS=V3)
test_word_4g_count <- ngram_predictions[[2]] %>% 
    rename(word_4g_count_EAP=V1, word_4g_count_HPL=V2, word_4g_count_MWS=V3)
metrics_word_4g_count <- ngram_predictions[[3]] %>% 
    rename(logloss=V1, acc=V2)
rm(ngram_df, ngram_predictions)
invisible(gc())
metrics_word_4g_count
```

### Character-based N-grams {.tabset .tabset-fade .tabset-pills}

```{r, cache = T}
get_char_ngram_df <- function(in_token, in_n, in_cnt){
  
  if(in_n <= 3){
    
      df <- map_df(1:in_n, ~ unnest_tokens(train %>% select(-author), token, text, 
                                        token = in_token, n = .x, to_lower = F, 
                                        lowercase = F, strip_non_alphanum = F)) %>%
          select(-id) %>%
          count(token) %>%
          filter(n > in_n) %>% 
          .$token

      df_cnt <- map_df(1:in_n, ~ unnest_tokens(train %>% select(-author) %>% bind_rows(test), token, text, 
                                                 token = in_token, n = .x, to_lower = F, 
                                                 lowercase = F, strip_non_alphanum = F)) %>%
          count(id, token) %>%
          filter(token %in% df) %>%
          cast_dtm(id, token, n) %>%
          as.matrix()
  }else{
  
      df <- train %>%
          select(-author) %>% 
          unnest_tokens(token, text, token = in_token, n = in_n, to_lower = F, lowercase = F, strip_non_alphanum = F) %>% 
          select(-id) %>%
          count(token) %>%
          filter(n > in_n) %>%
          .$token
      
      df_cnt <- train %>%
          select(-author) %>%
          bind_rows(test) %>%
          unnest_tokens(token, text, token = in_token, n = in_n, to_lower = F, lowercase = F, strip_non_alphanum = F) %>%
          count(id, token) %>%
          filter(token %in% df) %>%
          cast_dtm(id, token, n)
  }
  
    if(in_cnt != 0){
        rowsRemoved <- setdiff(c(train$id,test$id),rownames(df_cnt))
        allZeros <- matrix(0, length(rowsRemoved), ncol(df_cnt), 
                           dimnames = list(rowsRemoved, colnames(df_cnt)))
        df_cnt <- df_cnt %>% rbind(allZeros)
        rm(rowsRemoved,allZeros)
    }
    
    x_train_df <- df_cnt[train$id,] %>% as.matrix
    x_test_df <- df_cnt[test$id,] %>% as.matrix
    rm(df, df_cnt)
        
    return(list(x_train_df, x_test_df))
}
```

#### N <= 3

```{r, message = F, warning = F, cache = T}
ngram_df <- get_char_ngram_df(in_token = 'character_shingles', in_n = 3, in_cnt = 0)
ngram_predictions <- get_ngram_predictions(ngram_df, 'char_3g_count')
train_char_3g_count <- ngram_predictions[[1]] %>%  
    rename(char_3g_count_EAP=V1, char_3g_count_HPL=V2, char_3g_count_MWS=V3)
test_char_3g_count <- ngram_predictions[[2]] %>% 
    rename(char_3g_count_EAP=V1, char_3g_count_HPL=V2, char_3g_count_MWS=V3)
metrics_char_3g_count <- ngram_predictions[[3]] %>% 
    rename(logloss=V1, acc=V2)
rm(ngram_df, ngram_predictions)
invisible(gc())
metrics_char_3g_count
```

#### N = 4

```{r, message = F, warning = F, cache = T}
ngram_df <- get_char_ngram_df(in_token = 'character_shingles', in_n = 4, in_cnt = 2)
ngram_predictions <- get_ngram_predictions(ngram_df, 'char_4g_count')
train_char_4g_count <- ngram_predictions[[1]] %>%  
    rename(char_4g_count_EAP=V1, char_4g_count_HPL=V2, char_4g_count_MWS=V3)
test_char_4g_count <- ngram_predictions[[2]] %>% 
    rename(char_4g_count_EAP=V1, char_4g_count_HPL=V2, char_4g_count_MWS=V3)
metrics_char_4g_count <- ngram_predictions[[3]] %>% 
    rename(logloss=V1, acc=V2)
rm(ngram_df, ngram_predictions)
invisible(gc())
metrics_char_4g_count
```

#### N = 5

```{r, message = F, warning = F, cache = T}
ngram_df <- get_char_ngram_df(in_token = 'character_shingles', in_n = 5, in_cnt = 2)
ngram_predictions <- get_ngram_predictions(ngram_df, 'char_5g_count')
train_char_5g_count <- ngram_predictions[[1]] %>%  
    rename(char_5g_count_EAP=V1, char_5g_count_HPL=V2, char_5g_count_MWS=V3)
test_char_5g_count <- ngram_predictions[[2]] %>% 
    rename(char_5g_count_EAP=V1, char_5g_count_HPL=V2, char_5g_count_MWS=V3)
metrics_char_5g_count <- ngram_predictions[[3]] %>% 
    rename(logloss=V1, acc=V2)
rm(ngram_df, ngram_predictions)
invisible(gc())
metrics_char_5g_count
```

#### N = 6

```{r, message = F, warning = F, cache = T}
ngram_df <- get_char_ngram_df(in_token = 'character_shingles', in_n = 6, in_cnt = 2)
ngram_predictions <- get_ngram_predictions(ngram_df, 'char_6g_count')
train_char_6g_count <- ngram_predictions[[1]] %>%  
    rename(char_6g_count_EAP=V1, char_6g_count_HPL=V2, char_6g_count_MWS=V3)
test_char_6g_count <- ngram_predictions[[2]] %>% 
    rename(char_6g_count_EAP=V1, char_6g_count_HPL=V2, char_6g_count_MWS=V3)
metrics_char_6g_count <- ngram_predictions[[3]] %>% 
    rename(logloss=V1, acc=V2)
rm(ngram_df, ngram_predictions)
invisible(gc())
metrics_char_6g_count
```

### POS-tagged NN

```{r, message = F, warning = F, cache = T}
train_tag <- train_pos_tagged %>% 
    mutate(pos = str_replace_all(.$xpos, '\\$', 'S')) %>%
    group_by(doc_id) %>%
    summarise(text_tag = str_c(pos, collapse = ' '))

test_tag <- test_pos_tagged %>% 
    mutate(pos = str_replace_all(.$xpos, '\\$', 'S')) %>%
    group_by(doc_id) %>%
    summarise(text_tag = str_c(pos, collapse = ' '))

df <- map_df(1:4, ~ unnest_tokens(train_tag, token, text_tag, 
                                        token = 'ngrams', to_lower = F, n = .x)) %>%
    count(token) %>%
    filter(n > 2) %>%
    .$token

df_cnt <- map_df(1:4, ~ unnest_tokens(train_tag %>% bind_rows(test_tag), token, 
                                              text_tag, token = 'ngrams', to_lower = F, n = .x)) %>%
    rename(id = doc_id) %>% 
    count(id, token) %>%
    filter(token %in% df) %>%
    cast_dtm(id, token, n) %>%
    as.matrix()

x_train_df <- df_cnt[train$id,] %>% as.matrix
x_test_df <- df_cnt[test$id,] %>% as.matrix
rm(df, df_cnt)

ngram_predictions <- get_ngram_predictions(list(x_train_df, x_test_df), 'pos_count')
train_pos_count <- ngram_predictions[[1]] %>%  
    rename(pos_count_EAP=V1, pos_count_HPL=V2, pos_count_MWS=V3)
test_pos_count <- ngram_predictions[[2]] %>% 
    rename(pos_count_EAP=V1, pos_count_HPL=V2, pos_count_MWS=V3)
metrics_pos_count <- ngram_predictions[[3]] %>% 
    rename(logloss=V1, acc=V2)
rm(ngram_df, ngram_predictions)
invisible(gc())
metrics_pos_count
```

## Embedding-based NN

### Custom Embeddings

```{r, cache = T}
max_features <- 10000

tokenizer <- text_tokenizer(num_words = max_features, lower = T) %>%
    fit_text_tokenizer(train$text)  
train_seq <- texts_to_sequences(tokenizer, train$text)
test_seq <- texts_to_sequences(tokenizer, test$text)
max_len <- 60
train_seq_padded <- pad_sequences(train_seq, maxlen = max_len)
test_seq_padded <- pad_sequences(test_seq, maxlen = max_len)
```

### LSTM

```{r, message = F, warning = F, cache = T}
set.seed(42)

model <- keras_model_sequential() %>% 
    layer_embedding(input_dim = max_features + 1, output_dim = 45, input_length = max_len) %>%
    layer_dropout(rate = 0.2) %>%
    layer_lstm(units = 45, recurrent_dropout = 0.2, dropout = 0.2) %>%
    layer_dense(units = 16, activation = 'relu') %>%
    layer_dropout(rate = 0.2) %>%
    layer_dense(units = 3, activation = 'softmax') %>% 
    compile(
        loss = 'binary_crossentropy',
        optimizer = 'rmsprop',
        metrics = 'accuracy'
        )
    
    
lstm <- model %>%
    fit(
      train_seq_padded, y_train,
      batch_size = 2^9, 
      epochs = 20,
      validation_split = 0.1,
      verbose = T,
      callbacks = list(
          callback_early_stopping(monitor = 'val_loss', patience = 2),
          callback_model_checkpoint(
              filepath = 'word_vec_lstm.hdf5',
              monitor = 'val_loss',
              save_best_only = TRUE)
          )
      )

lstm_m <- load_model_hdf5('word_vec_lstm.hdf5')

test_pred_lstm <- lstm_m %>%
    predict(test_seq_padded) %>%
    as.data.frame() %>% 
    rename(lstm_EAP=V1, lstm_HPL=V2, lstm_MWS=V3)

train_pred_lstm <- lstm_m %>%
    predict(train_seq_padded) %>%
    as.data.frame() %>% 
    rename(lstm_EAP=V1, lstm_HPL=V2, lstm_MWS=V3)
    
fold_eval <- lstm_m %>% evaluate(train_seq_padded, y_train)

read_csv(file = 'data/sample_submission.csv') %>% 
    select(id) %>% 
    bind_cols(., test_pred_lstm %>% as_tibble()) %>% 
    rename(EAP = lstm_EAP, HPL = lstm_HPL, MWS = lstm_MWS) %>% 
    write_csv('submissions/sub_lstm.csv')

plot(lstm)
```

### BI-LSTM

```{r, message = F, warning = F, cache = T}
set.seed(42)

model <- keras_model_sequential() %>% 
    layer_embedding(input_dim = max_features, output_dim = 45, input_length = max_len) %>%
    bidirectional(layer_lstm(units = 45, recurrent_dropout = 0.2, return_sequences = T)) %>%
    layer_global_max_pooling_1d() %>% 
    layer_dropout(rate = 0.2) %>% 
    layer_dense(units = 3, activation = 'softmax') %>% 
    compile(
        loss = "binary_crossentropy",
        optimizer = "rmsprop",
        metrics = "accuracy"
        )
    
bilstm <- model %>%
    fit(
      train_seq_padded, y_train,
      batch_size = 2^9, 
      epochs = 20,
      validation_split = 0.1,
      verbose = T,
      callbacks = list(
          callback_early_stopping(monitor = 'val_loss', patience = 2),
          callback_model_checkpoint(
              filepath = 'word_vec_bilstm.hdf5',
              monitor = 'val_loss',
              save_best_only = T)
          )
      )

bilstm_m <- load_model_hdf5('word_vec_bilstm.hdf5')

test_pred_bilstm <- bilstm_m %>%
    predict(test_seq_padded) %>%
    as.data.frame() %>% 
    rename(bi_lstm_EAP=V1, bi_lstm_HPL=V2, bi_lstm_MWS=V3)

train_pred_bilstm <- bilstm_m %>%
    predict(train_seq_padded) %>%
    as.data.frame() %>% 
    rename(bi_lstm_EAP=V1, bi_lstm_HPL=V2, bi_lstm_MWS=V3)


fold_eval <- bilstm_m %>% evaluate(train_seq_padded, y_train)

read_csv(file = 'data/sample_submission.csv') %>% 
    select(id) %>% 
    bind_cols(., test_pred_bilstm %>% as_tibble()) %>% 
    rename(EAP = bi_lstm_EAP, HPL = bi_lstm_HPL, MWS = bi_lstm_MWS) %>% 
    write_csv('submissions/sub_bilstm.csv')

plot(bilstm)
```

## Ensemble

```{r, cache = T}
train_all_sp <- train %>% select(id, author) %>% 
    bind_cols(train_word_1g_count) %>%
    bind_cols(train_word_2g_count) %>%
    bind_cols(train_word_3g_count) %>%
    bind_cols(train_word_4g_count) %>% 
    bind_cols(train_char_3g_count) %>% 
    bind_cols(train_char_4g_count) %>% 
    bind_cols(train_char_5g_count) %>% 
    bind_cols(train_char_6g_count) %>% 
    bind_cols(train_pos_count) %>% 
    bind_cols(train_pred_lstm) %>%   
    bind_cols(train_pred_bilstm)


test_all_sp <- test %>% select(id) %>% 
    bind_cols(test_word_1g_count) %>%
    bind_cols(test_word_2g_count) %>%
    bind_cols(test_word_3g_count) %>%
    bind_cols(test_word_4g_count) %>% 
    bind_cols(test_char_3g_count) %>% 
    bind_cols(test_char_4g_count) %>% 
    bind_cols(test_char_5g_count) %>% 
    bind_cols(test_char_6g_count) %>% 
    bind_cols(test_pos_count) %>% 
    bind_cols(test_pred_lstm) %>%   
    bind_cols(test_pred_bilstm)

train_all_sp %>%
    select(-c(id)) %>%
    group_by(author) %>%
    summarise_all(mean) %>%
    gather(feature, value, -author) %>%
    ggplot(aes(x = feature, y = value, color = author, fill = author, size = value)) +
    scale_color_manual(values = c(HPL = 'blue4', EAP = 'red4', MWS = 'purple4')) +
    geom_point(alpha = 0.6, size = 3) +
    labs(x = 'feature', y = 'mean value by author') +
    coord_flip() +
    theme_bw()
```

```{r, cache = T}
dtrain <- train_total %>% 
    left_join(train_all_sp, by = c('id', 'author'))
dtrain[is.na(dtrain)] <- 0

dtest <- test_total %>% 
    left_join(test_all_sp, by = 'id')
dtest[is.na(dtest)] <- 0
dim(dtrain)
dim(dtest)
```

### NN features correlation

```{r fig.height=10, fig.width=10, cache = T}
sCorr <- cor(dtrain %>% select(-id, -author), method = 'spearman')
sigCorr <- findCorrelation(sCorr, cutoff = .8, names = TRUE)
ggcorrplot(cor(dtrain %>% select(sigCorr), method = 'spearman'))
```

### XGBoost

```{r, message = F, warning = F, cache = T, results='hide'}
customSummary <- function(data, lev = levels(data$obs), model = NULL) {
    mcs <- multiClassSummary(data, lev = levels(data$obs), model = NULL)
    mnll <- mnLogLoss(data, lev = levels(data$obs), model = NULL)
    out <- c(mnll, mcs['Accuracy'])
}


set.seed(42)
xgbt <- train(x = dtrain %>% select(-c(id, author)), 
                  y = factor(dtrain$author),
                  method = 'xgbTree',
                  metric = 'logLoss',
                  tuneGrid = expand.grid(nrounds = seq(100,1400,100),
                                         max_depth = 4, 
                                         eta = 0.02,
                                         gamma = 0.5,
                                         colsample_bytree = 0.35,
                                         min_child_weight = 4,
                                         subsample = 0.85),
                  trControl = trainControl(method = "cv",
                                           number = 10,
                                           classProbs = TRUE,
                                           summaryFunction = customSummary,
                                           search = 'grid')
)
```

## Final submission

```{r, message = F, warning = F, cache = T}
xgbt
sub_xgbt <- read_csv('data/sample_submission.csv') %>%
    select(id) %>%
    bind_cols(predict(xgbt, dtest %>% select(-c(id)), type = 'prob'))
write_excel_csv(sub_xgbt, 'submissions/stacked_xgb_short.csv')
```